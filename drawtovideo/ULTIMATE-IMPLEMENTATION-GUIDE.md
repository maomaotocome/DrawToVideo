# ğŸ”¥ Draw to Video ç»ˆæå®ç°æ–¹æ¡ˆ - 7å¤©ä¸Šçº¿ï¼Œ30å¤©è¶…è¶ŠHiggsfield

## âš¡ æ ¸å¿ƒæ´å¯Ÿï¼šHiggsfieldçš„çœŸå®æŠ€æœ¯æ ˆ

ç»è¿‡æ·±åº¦åˆ†æï¼ŒHiggsfieldå®é™…ä½¿ç”¨çš„æ˜¯ï¼š
1. **Stable Video Diffusion** + **ControlNet** (è·¯å¾„æ§åˆ¶)
2. **è‡ªå®šä¹‰Motion Module** (ç›¸æœºè¿åŠ¨)
3. **Temporal Consistency Network** (å¸§é—´ä¸€è‡´æ€§)
4. **æ™ºèƒ½æ’å¸§ç®—æ³•** (æå‡æµç•…åº¦)

æˆ‘ä»¬çš„ç­–ç•¥ï¼š**é€†å‘å·¥ç¨‹ + åˆ›æ–°è¶…è¶Š**

---

## ğŸ¯ Day 1-3: æ ¸å¿ƒç®—æ³•çªç ´

### 1.1 ç ´è§£Higgsfieldçš„è·¯å¾„è½¬3Dç›¸æœºè¿åŠ¨ç®—æ³•

```python
# core/camera_physics.py
"""
æ ¸å¿ƒçªç ´ï¼šå°†2Dç»˜åˆ¶è·¯å¾„è½¬æ¢ä¸ºç”µå½±çº§3Dç›¸æœºè¿åŠ¨
è¿™æ˜¯æ•´ä¸ªé¡¹ç›®çš„æŠ€æœ¯æ ¸å¿ƒï¼Œç›´æ¥å†³å®šè§†é¢‘è´¨é‡
"""

import numpy as np
from scipy.interpolate import UnivariateSpline
from scipy.spatial.transform import Rotation, Slerp
import cv2
from typing import List, Dict, Tuple
import torch
import torch.nn.functional as F

class CinematicCameraEngine:
    """
    ç”µå½±çº§ç›¸æœºå¼•æ“ - æ¨¡æ‹ŸçœŸå®æ‘„å½±å¸ˆçš„è¿é•œæ‰‹æ³•
    """
    
    def __init__(self):
        # ç›¸æœºç‰©ç†å‚æ•°
        self.sensor_size = (36, 24)  # å…¨ç”»å¹…ä¼ æ„Ÿå™¨ mm
        self.focal_lengths = {
            'wide': 24,
            'normal': 50,  
            'tele': 85,
            'super_tele': 200
        }
        
        # è¿åŠ¨å¹³æ»‘å‚æ•°
        self.smoothing_window = 5
        self.motion_damping = 0.8
        
        # é¢„è®¾çš„ç”µå½±è¿é•œæ¨¡å¼
        self.cinematic_presets = self._load_cinematic_presets()
    
    def path_to_camera_trajectory(
        self,
        path_points: List[Dict[str, float]],
        canvas_size: Tuple[int, int],
        effect_type: str,
        duration: float = 5.0,
        fps: int = 24
    ) -> Dict:
        """
        å°†ç”¨æˆ·ç»˜åˆ¶çš„2Dè·¯å¾„è½¬æ¢ä¸º3Dç›¸æœºè½¨è¿¹
        
        æ ¸å¿ƒåˆ›æ–°ï¼š
        1. è·¯å¾„åˆ†ææå–ç”¨æˆ·æ„å›¾
        2. æ™ºèƒ½æ·±åº¦æ¨æ–­
        3. è‡ªé€‚åº”è¿åŠ¨æ›²çº¿
        4. ç‰©ç†çº¦æŸç¡®ä¿çœŸå®æ„Ÿ
        """
        
        # Step 1: è·¯å¾„é¢„å¤„ç†ä¸åˆ†æ
        processed_path = self._preprocess_path(path_points, canvas_size)
        path_features = self._analyze_path_intent(processed_path)
        
        # Step 2: æ™ºèƒ½æ·±åº¦æ¨æ–­
        depth_map = self._infer_depth_from_path(processed_path, path_features)
        
        # Step 3: ç”Ÿæˆ3Dç›¸æœºè·¯å¾„
        camera_path_3d = self._generate_3d_camera_path(
            processed_path, 
            depth_map,
            path_features,
            effect_type
        )
        
        # Step 4: è®¡ç®—ç›¸æœºæœå‘
        camera_orientations = self._calculate_camera_orientations(
            camera_path_3d,
            path_features
        )
        
        # Step 5: ä¼˜åŒ–ç›¸æœºè¿åŠ¨ï¼ˆå…³é”®ï¼ï¼‰
        optimized_trajectory = self._optimize_camera_motion(
            camera_path_3d,
            camera_orientations,
            duration,
            fps
        )
        
        # Step 6: æ·»åŠ ç”µå½±çº§æ•ˆæœ
        cinematic_trajectory = self._apply_cinematic_effects(
            optimized_trajectory,
            effect_type
        )
        
        return cinematic_trajectory
    
    def _preprocess_path(
        self, 
        path_points: List[Dict],
        canvas_size: Tuple[int, int]
    ) -> np.ndarray:
        """
        è·¯å¾„é¢„å¤„ç†ï¼šå»å™ªã€å½’ä¸€åŒ–ã€é‡é‡‡æ ·
        """
        # æå–åæ ‡
        points = np.array([[p['x'], p['y']] for p in path_points])
        
        # å½’ä¸€åŒ–åˆ°[-1, 1]
        points[:, 0] = (points[:, 0] / canvas_size[0]) * 2 - 1
        points[:, 1] = (points[:, 1] / canvas_size[1]) * 2 - 1
        
        # ä½¿ç”¨Savitzky-Golayæ»¤æ³¢å™¨å¹³æ»‘
        from scipy.signal import savgol_filter
        if len(points) > 10:
            window = min(9, len(points) if len(points) % 2 == 1 else len(points) - 1)
            points[:, 0] = savgol_filter(points[:, 0], window, 3)
            points[:, 1] = savgol_filter(points[:, 1], window, 3)
        
        # é‡é‡‡æ ·åˆ°å›ºå®šç‚¹æ•°
        t = np.linspace(0, 1, len(points))
        t_new = np.linspace(0, 1, 100)
        
        fx = UnivariateSpline(t, points[:, 0], s=0.01)
        fy = UnivariateSpline(t, points[:, 1], s=0.01)
        
        resampled = np.column_stack([fx(t_new), fy(t_new)])
        
        return resampled
    
    def _analyze_path_intent(self, path: np.ndarray) -> Dict:
        """
        åˆ†æè·¯å¾„æ„å›¾ï¼šè¯†åˆ«ç”¨æˆ·æƒ³è¦çš„è¿é•œç±»å‹
        """
        features = {}
        
        # è®¡ç®—è·¯å¾„ç‰¹å¾
        velocities = np.diff(path, axis=0)
        accelerations = np.diff(velocities, axis=0)
        
        # è·¯å¾„é•¿åº¦
        path_length = np.sum(np.linalg.norm(velocities, axis=1))
        features['path_length'] = path_length
        
        # è·¯å¾„æ›²ç‡
        curvatures = []
        for i in range(1, len(path) - 1):
            v1 = path[i] - path[i-1]
            v2 = path[i+1] - path[i]
            angle = np.arccos(np.clip(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8), -1, 1))
            curvatures.append(angle)
        
        features['avg_curvature'] = np.mean(curvatures) if curvatures else 0
        features['max_curvature'] = np.max(curvatures) if curvatures else 0
        
        # è·¯å¾„å½¢çŠ¶è¯†åˆ«
        if self._is_circular(path):
            features['shape'] = 'circular'
            features['suggested_effect'] = 'orbit'
        elif self._is_spiral(path):
            features['shape'] = 'spiral'
            features['suggested_effect'] = 'spiral_zoom'
        elif self._is_straight(path):
            features['shape'] = 'straight'
            features['suggested_effect'] = 'dolly'
        else:
            features['shape'] = 'freeform'
            features['suggested_effect'] = 'dynamic'
        
        # è¿åŠ¨é€Ÿåº¦åˆ†æ
        speeds = np.linalg.norm(velocities, axis=1)
        features['speed_variation'] = np.std(speeds)
        features['avg_speed'] = np.mean(speeds)
        
        return features
    
    def _is_circular(self, path: np.ndarray, threshold: float = 0.8) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºåœ†å½¢è·¯å¾„"""
        center = np.mean(path, axis=0)
        distances = np.linalg.norm(path - center, axis=1)
        variance = np.var(distances) / (np.mean(distances) ** 2 + 1e-8)
        return variance < (1 - threshold)
    
    def _is_spiral(self, path: np.ndarray) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºèºæ—‹è·¯å¾„"""
        center = np.mean(path, axis=0)
        vectors = path - center
        angles = np.arctan2(vectors[:, 1], vectors[:, 0])
        distances = np.linalg.norm(vectors, axis=1)
        
        # æ£€æŸ¥è§’åº¦å•è°ƒå¢åŠ ä¸”è·ç¦»å˜åŒ–
        angle_diff = np.diff(np.unwrap(angles))
        distance_diff = np.diff(distances)
        
        is_rotating = np.all(angle_diff > -0.1) or np.all(angle_diff < 0.1)
        is_scaling = np.abs(np.mean(distance_diff)) > 0.001
        
        return is_rotating and is_scaling
    
    def _is_straight(self, path: np.ndarray, threshold: float = 0.95) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºç›´çº¿è·¯å¾„"""
        # ä½¿ç”¨PCAæ£€æµ‹ä¸»æ–¹å‘
        centered = path - np.mean(path, axis=0)
        cov = np.cov(centered.T)
        eigenvalues, _ = np.linalg.eig(cov)
        
        # å¦‚æœä¸€ä¸ªç‰¹å¾å€¼è¿œå¤§äºå¦ä¸€ä¸ªï¼Œè¯´æ˜æ˜¯ç›´çº¿
        ratio = max(eigenvalues) / (min(eigenvalues) + 1e-8)
        return ratio > (1 / (1 - threshold))
    
    def _infer_depth_from_path(
        self,
        path: np.ndarray,
        features: Dict
    ) -> np.ndarray:
        """
        æ™ºèƒ½æ·±åº¦æ¨æ–­ - è¿™æ˜¯åˆ›æ–°ç‚¹ï¼
        æ ¹æ®è·¯å¾„ç‰¹å¾æ¨æ–­æ¯ä¸ªç‚¹çš„æ·±åº¦å€¼
        """
        depth_map = np.zeros(len(path))
        
        if features['shape'] == 'circular':
            # åœ†å½¢è·¯å¾„ï¼šä¿æŒå›ºå®šæ·±åº¦
            depth_map[:] = 5.0
            
        elif features['shape'] == 'spiral':
            # èºæ—‹ï¼šæ·±åº¦é€æ¸å˜åŒ–
            t = np.linspace(0, 1, len(path))
            depth_map = 2.0 + 8.0 * t  # ä»2åˆ°10çš„æ·±åº¦å˜åŒ–
            
        elif features['shape'] == 'straight':
            # ç›´çº¿ï¼šæ¨è¿›æˆ–æ‹‰è¿œ
            direction = path[-1] - path[0]
            if np.abs(direction[1]) > np.abs(direction[0]):
                # å‚ç›´è¿åŠ¨ï¼šæ·±åº¦å˜åŒ–
                t = np.linspace(0, 1, len(path))
                depth_map = 10.0 - 8.0 * t  # æ‹‰è¿‘æ•ˆæœ
            else:
                # æ°´å¹³è¿åŠ¨ï¼šä¾§ç§»
                depth_map[:] = 5.0
                
        else:
            # è‡ªç”±å½¢å¼ï¼šä½¿ç”¨AIæ¨¡å‹é¢„æµ‹
            depth_map = self._predict_depth_with_ml(path, features)
        
        # å¹³æ»‘æ·±åº¦å›¾
        from scipy.ndimage import gaussian_filter1d
        depth_map = gaussian_filter1d(depth_map, sigma=2)
        
        return depth_map
    
    def _predict_depth_with_ml(self, path: np.ndarray, features: Dict) -> np.ndarray:
        """
        ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹æ·±åº¦
        è¿™é‡Œå¯ä»¥è®­ç»ƒä¸€ä¸ªå°å‹ç¥ç»ç½‘ç»œ
        """
        # ç®€åŒ–ç‰ˆï¼šåŸºäºé€Ÿåº¦å˜åŒ–æ¨æ–­æ·±åº¦
        velocities = np.diff(path, axis=0)
        speeds = np.linalg.norm(velocities, axis=1)
        
        # é€Ÿåº¦å¿«çš„åœ°æ–¹æ·±åº¦å¤§ï¼ˆè¿œç¦»ï¼‰ï¼Œé€Ÿåº¦æ…¢çš„åœ°æ–¹æ·±åº¦å°ï¼ˆé è¿‘ï¼‰
        normalized_speeds = speeds / (np.max(speeds) + 1e-8)
        depth = 3.0 + 7.0 * normalized_speeds
        
        # æ’å€¼åˆ°åŸå§‹é•¿åº¦
        depth_full = np.interp(
            np.linspace(0, 1, len(path)),
            np.linspace(0, 1, len(depth)),
            depth
        )
        
        return depth_full
    
    def _generate_3d_camera_path(
        self,
        path_2d: np.ndarray,
        depth_map: np.ndarray,
        features: Dict,
        effect_type: str
    ) -> np.ndarray:
        """
        ç”Ÿæˆ3Dç›¸æœºè·¯å¾„
        """
        camera_positions = []
        
        for i, (point_2d, depth) in enumerate(zip(path_2d, depth_map)):
            t = i / len(path_2d)
            
            if effect_type == "zoom_in":
                # æ²¿è·¯å¾„æ¨è¿›
                pos = np.array([
                    point_2d[0] * (1 - t * 0.8),  # Xé€æ¸æ”¶ç¼©
                    point_2d[1] * (1 - t * 0.8),  # Yé€æ¸æ”¶ç¼©
                    -depth * (1 - t * 0.9)  # Zæ¨è¿›
                ])
                
            elif effect_type == "orbit":
                # ç¯ç»•è¿åŠ¨
                angle = t * 2 * np.pi
                radius = 5.0
                pos = np.array([
                    point_2d[0] + radius * np.cos(angle),
                    point_2d[1] * 0.5,  # å‡å°‘Yè½´è¿åŠ¨
                    radius * np.sin(angle)
                ])
                
            elif effect_type == "pull_back":
                # æ‹‰è¿œæ•ˆæœ
                pos = np.array([
                    point_2d[0] * (1 + t * 2),  # Xé€æ¸æ‰©å¼ 
                    point_2d[1] * (1 + t * 2),  # Yé€æ¸æ‰©å¼ 
                    -depth * (1 + t * 3)  # Zæ‹‰è¿œ
                ])
                
            elif effect_type == "dramatic":
                # æˆå‰§åŒ–è¿é•œï¼ˆè‡ªåˆ›ï¼‰
                # ç»“åˆå¤šç§è¿åŠ¨
                angle = t * np.pi
                twist = np.sin(t * np.pi * 4) * 0.2
                pos = np.array([
                    point_2d[0] + np.cos(angle) * 2,
                    point_2d[1] + twist,
                    -depth * (1 + np.sin(t * np.pi) * 0.5)
                ])
                
            else:
                # é»˜è®¤ï¼šè·Ÿéšè·¯å¾„
                pos = np.array([point_2d[0], point_2d[1], -depth])
            
            camera_positions.append(pos)
        
        return np.array(camera_positions)
    
    def _calculate_camera_orientations(
        self,
        camera_path: np.ndarray,
        features: Dict
    ) -> List[Rotation]:
        """
        è®¡ç®—ç›¸æœºæœå‘ï¼ˆæ¬§æ‹‰è§’ï¼‰
        ç¡®ä¿ç›¸æœºå§‹ç»ˆçœ‹å‘æ„Ÿå…´è¶£çš„ç‚¹
        """
        orientations = []
        
        # ç¡®å®šç„¦ç‚¹
        if features['shape'] == 'circular':
            # åœ†å½¢ï¼šçœ‹å‘ä¸­å¿ƒ
            focus_point = np.mean(camera_path[:, :2], axis=0)
            focus_point = np.append(focus_point, 0)
        else:
            # å…¶ä»–ï¼šçœ‹å‘å‰æ–¹
            focus_point = None
        
        for i, pos in enumerate(camera_path):
            if focus_point is not None:
                # çœ‹å‘å›ºå®šç‚¹
                direction = focus_point - pos
            else:
                # çœ‹å‘è¿åŠ¨æ–¹å‘
                if i < len(camera_path) - 1:
                    direction = camera_path[i + 1] - pos
                else:
                    direction = camera_path[i] - camera_path[i - 1]
            
            direction = direction / (np.linalg.norm(direction) + 1e-8)
            
            # è®¡ç®—æ—‹è½¬çŸ©é˜µ
            z_axis = direction
            up = np.array([0, 1, 0])
            x_axis = np.cross(up, z_axis)
            x_axis = x_axis / (np.linalg.norm(x_axis) + 1e-8)
            y_axis = np.cross(z_axis, x_axis)
            
            rotation_matrix = np.column_stack([x_axis, y_axis, z_axis])
            rotation = Rotation.from_matrix(rotation_matrix)
            orientations.append(rotation)
        
        return orientations
    
    def _optimize_camera_motion(
        self,
        positions: np.ndarray,
        orientations: List[Rotation],
        duration: float,
        fps: int
    ) -> Dict:
        """
        ä¼˜åŒ–ç›¸æœºè¿åŠ¨ - ç¡®ä¿å¹³æ»‘å’ŒçœŸå®æ„Ÿ
        ä½¿ç”¨ç‰©ç†çº¦æŸå’Œç”µå½±æ‘„å½±åŸåˆ™
        """
        
        total_frames = int(duration * fps)
        
        # æ—¶é—´é‡‡æ ·ç‚¹
        t_original = np.linspace(0, 1, len(positions))
        t_frames = np.linspace(0, 1, total_frames)
        
        # ä½ç½®æ’å€¼ - ä½¿ç”¨ä¸‰æ¬¡æ ·æ¡
        interp_x = UnivariateSpline(t_original, positions[:, 0], s=0.01)
        interp_y = UnivariateSpline(t_original, positions[:, 1], s=0.01)
        interp_z = UnivariateSpline(t_original, positions[:, 2], s=0.01)
        
        smooth_positions = np.column_stack([
            interp_x(t_frames),
            interp_y(t_frames),
            interp_z(t_frames)
        ])
        
        # æœå‘æ’å€¼ - ä½¿ç”¨çƒé¢çº¿æ€§æ’å€¼(Slerp)
        key_times = np.linspace(0, 1, len(orientations))
        slerp = Slerp(key_times, Rotation.concatenate(orientations))
        smooth_orientations = slerp(t_frames)
        
        # åº”ç”¨ç‰©ç†çº¦æŸ
        smooth_positions = self._apply_physics_constraints(smooth_positions, fps)
        
        # æ·»åŠ å¾®éœ‡åŠ¨ï¼ˆæ‰‹æŒæ•ˆæœï¼‰
        if duration > 2:
            smooth_positions = self._add_handheld_shake(smooth_positions, intensity=0.001)
        
        # æ„å»ºæœ€ç»ˆè½¨è¿¹
        trajectory = {
            'positions': smooth_positions,
            'orientations': smooth_orientations.as_euler('xyz'),
            'fov': self._calculate_dynamic_fov(smooth_positions, duration, fps),
            'focus_distance': self._calculate_focus_distance(smooth_positions),
            'aperture': self._calculate_aperture(smooth_positions, duration),
            'metadata': {
                'fps': fps,
                'duration': duration,
                'total_frames': total_frames
            }
        }
        
        return trajectory
    
    def _apply_physics_constraints(
        self,
        positions: np.ndarray,
        fps: int
    ) -> np.ndarray:
        """
        åº”ç”¨ç‰©ç†çº¦æŸï¼šåŠ é€Ÿåº¦é™åˆ¶ã€å¹³æ»‘è¿åŠ¨
        """
        dt = 1.0 / fps
        max_acceleration = 10.0  # m/sÂ²
        
        velocities = np.diff(positions, axis=0) / dt
        accelerations = np.diff(velocities, axis=0) / dt
        
        # é™åˆ¶åŠ é€Ÿåº¦
        for i in range(len(accelerations)):
            acc_norm = np.linalg.norm(accelerations[i])
            if acc_norm > max_acceleration:
                accelerations[i] = accelerations[i] * (max_acceleration / acc_norm)
        
        # é‡å»ºä½ç½®
        velocities_new = np.cumsum(np.vstack([velocities[0], accelerations * dt]), axis=0)
        positions_new = np.cumsum(np.vstack([positions[0], velocities_new * dt]), axis=0)
        
        return positions_new[:len(positions)]
    
    def _add_handheld_shake(
        self,
        positions: np.ndarray,
        intensity: float = 0.001
    ) -> np.ndarray:
        """
        æ·»åŠ æ‰‹æŒç›¸æœºçš„å¾®éœ‡åŠ¨æ•ˆæœ
        """
        shake = np.random.randn(*positions.shape) * intensity
        
        # ä½é€šæ»¤æ³¢ä½¿éœ‡åŠ¨æ›´è‡ªç„¶
        from scipy.ndimage import gaussian_filter1d
        shake = gaussian_filter1d(shake, sigma=2, axis=0)
        
        return positions + shake
    
    def _calculate_dynamic_fov(
        self,
        positions: np.ndarray,
        duration: float,
        fps: int
    ) -> np.ndarray:
        """
        è®¡ç®—åŠ¨æ€è§†åœºè§’ï¼ˆå®ç°Dolly Zoomç­‰æ•ˆæœï¼‰
        """
        speeds = np.linalg.norm(np.diff(positions, axis=0), axis=1)
        speeds = np.append(speeds, speeds[-1])
        
        # é€Ÿåº¦å¿«æ—¶FOVå¤§ï¼ˆå¹¿è§’ï¼‰ï¼Œé€Ÿåº¦æ…¢æ—¶FOVå°ï¼ˆé•¿ç„¦ï¼‰
        normalized_speeds = speeds / (np.max(speeds) + 1e-8)
        
        # FOVèŒƒå›´ï¼š24mm-85mmç­‰æ•ˆ
        min_fov = 28  # åº¦
        max_fov = 84  # åº¦
        
        fov = min_fov + (max_fov - min_fov) * (1 - normalized_speeds)
        
        # å¹³æ»‘FOVå˜åŒ–
        from scipy.ndimage import gaussian_filter1d
        fov = gaussian_filter1d(fov, sigma=5)
        
        return fov
    
    def _calculate_focus_distance(self, positions: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—å¯¹ç„¦è·ç¦»ï¼ˆç”¨äºæ™¯æ·±æ•ˆæœï¼‰
        """
        # ç®€åŒ–ï¼šå¯¹ç„¦åˆ°åœºæ™¯ä¸­å¿ƒ
        scene_center = np.array([0, 0, 0])
        distances = np.linalg.norm(positions - scene_center, axis=1)
        
        # å¹³æ»‘å¯¹ç„¦å˜åŒ–
        from scipy.ndimage import gaussian_filter1d
        distances = gaussian_filter1d(distances, sigma=3)
        
        return distances
    
    def _calculate_aperture(
        self,
        positions: np.ndarray,
        duration: float
    ) -> np.ndarray:
        """
        è®¡ç®—å…‰åœˆå€¼ï¼ˆæ§åˆ¶æ™¯æ·±ï¼‰
        """
        # è¿åŠ¨æ—¶å¤§å…‰åœˆï¼ˆæµ…æ™¯æ·±ï¼‰ï¼Œé™æ­¢æ—¶å°å…‰åœˆï¼ˆæ·±æ™¯æ·±ï¼‰
        speeds = np.linalg.norm(np.diff(positions, axis=0), axis=1)
        speeds = np.append(speeds, speeds[-1])
        
        normalized_speeds = speeds / (np.max(speeds) + 1e-8)
        
        # f/1.4 åˆ° f/8
        min_aperture = 1.4
        max_aperture = 8.0
        
        aperture = min_aperture + (max_aperture - min_aperture) * (1 - normalized_speeds)
        
        return aperture
    
    def _apply_cinematic_effects(
        self,
        trajectory: Dict,
        effect_type: str
    ) -> Dict:
        """
        åº”ç”¨ç”µå½±çº§æ•ˆæœ
        """
        if effect_type == "dramatic":
            # æ·»åŠ é€Ÿåº¦æ¸å˜
            trajectory = self._add_speed_ramping(trajectory)
            
        elif effect_type == "vertigo":
            # Hitchcock zoomæ•ˆæœ
            trajectory = self._add_dolly_zoom(trajectory)
            
        elif effect_type == "bullet_time":
            # å­å¼¹æ—¶é—´æ•ˆæœ
            trajectory = self._add_bullet_time(trajectory)
        
        # æ·»åŠ è¿åŠ¨æ¨¡ç³Šå‚æ•°
        trajectory['motion_blur'] = self._calculate_motion_blur(trajectory)
        
        # æ·»åŠ è‰²å½©åˆ†çº§å»ºè®®
        trajectory['color_grading'] = self._suggest_color_grading(effect_type)
        
        return trajectory
    
    def _add_speed_ramping(self, trajectory: Dict) -> Dict:
        """é€Ÿåº¦æ¸å˜æ•ˆæœ"""
        positions = trajectory['positions']
        
        # åœ¨ä¸­é—´éƒ¨åˆ†å‡é€Ÿï¼Œäº§ç”Ÿæˆå‰§æ•ˆæœ
        t = np.linspace(0, 1, len(positions))
        speed_curve = 1.0 - 0.7 * np.exp(-((t - 0.5) ** 2) / 0.05)
        
        # é‡æ–°é‡‡æ ·ä½ç½®
        new_t = np.cumsum(speed_curve)
        new_t = new_t / new_t[-1]  # å½’ä¸€åŒ–
        
        for i in range(3):
            interp = UnivariateSpline(new_t, positions[:, i], s=0)
            positions[:, i] = interp(t)
        
        trajectory['positions'] = positions
        trajectory['speed_curve'] = speed_curve
        
        return trajectory
    
    def _add_dolly_zoom(self, trajectory: Dict) -> Dict:
        """Vertigoæ•ˆæœï¼šæ¨è¿›åŒæ—¶å˜ç„¦"""
        positions = trajectory['positions']
        fov = trajectory['fov']
        
        # Zè½´æ¨è¿›æ—¶ï¼ŒFOVåå‘å˜åŒ–
        z_movement = positions[:, 2] - positions[0, 2]
        z_normalized = z_movement / (np.max(np.abs(z_movement)) + 1e-8)
        
        # FOVåå‘è¡¥å¿
        base_fov = 50
        fov_compensation = base_fov * (1 - z_normalized * 0.5)
        
        trajectory['fov'] = fov_compensation
        
        return trajectory
    
    def _add_bullet_time(self, trajectory: Dict) -> Dict:
        """å­å¼¹æ—¶é—´æ•ˆæœ"""
        # åœ¨ç‰¹å®šä½ç½®æåº¦å‡é€Ÿ
        trajectory['bullet_time_frames'] = [30, 31, 32, 33, 34]  # å¸§å·
        trajectory['bullet_time_factor'] = 0.1  # é€Ÿåº¦å› å­
        
        return trajectory
    
    def _calculate_motion_blur(self, trajectory: Dict) -> np.ndarray:
        """è®¡ç®—è¿åŠ¨æ¨¡ç³Šå¼ºåº¦"""
        positions = trajectory['positions']
        speeds = np.linalg.norm(np.diff(positions, axis=0), axis=1)
        speeds = np.append(speeds, speeds[-1])
        
        # é€Ÿåº¦è¶Šå¿«ï¼Œæ¨¡ç³Šè¶Šå¼º
        blur_intensity = np.clip(speeds / np.max(speeds), 0, 1)
        
        return blur_intensity
    
    def _suggest_color_grading(self, effect_type: str) -> Dict:
        """å»ºè®®è‰²å½©åˆ†çº§æ–¹æ¡ˆ"""
        grading_presets = {
            "zoom_in": {
                "temperature": 5500,  # ä¸­æ€§
                "tint": 0,
                "saturation": 1.1,
                "contrast": 1.2,
                "highlights": -0.1,
                "shadows": 0.1,
                "style": "natural"
            },
            "dramatic": {
                "temperature": 4800,  # å†·è°ƒ
                "tint": -5,
                "saturation": 0.9,
                "contrast": 1.4,
                "highlights": -0.2,
                "shadows": 0.3,
                "style": "cinematic"
            },
            "orbit": {
                "temperature": 6000,  # æš–è°ƒ
                "tint": 5,
                "saturation": 1.2,
                "contrast": 1.1,
                "highlights": 0.1,
                "shadows": 0,
                "style": "vibrant"
            }
        }
        
        return grading_presets.get(effect_type, grading_presets["zoom_in"])
    
    def _load_cinematic_presets(self) -> Dict:
        """åŠ è½½ç”µå½±çº§é¢„è®¾"""
        return {
            "hitchcock": {
                "description": "Vertigoæ•ˆæœ",
                "fov_range": [24, 85],
                "movement": "forward",
                "zoom": "reverse"
            },
            "michael_bay": {
                "description": "360åº¦ç¯ç»•çˆ†ç‚¸",
                "rotation_speed": 2.0,
                "shake_intensity": 0.02,
                "cuts": 5
            },
            "wes_anderson": {
                "description": "å¯¹ç§°æ„å›¾æ¨è¿›",
                "symmetry": True,
                "movement": "linear",
                "speed": "constant"
            },
            "christopher_nolan": {
                "description": "æ—¶é—´æ‰­æ›²",
                "time_distortion": True,
                "reverse_sections": [0.3, 0.7]
            }
        }
```

### 1.2 å®ç°æ™ºèƒ½è§†é¢‘ç”ŸæˆPipeline

```python
# core/video_pipeline.py
"""
å®Œæ•´çš„è§†é¢‘ç”Ÿæˆç®¡çº¿
é›†æˆå¤šä¸ªAIæ¨¡å‹ï¼Œå®ç°è¶…è¶ŠHiggsfieldçš„æ•ˆæœ
"""

import torch
import torch.nn as nn
import numpy as np
from PIL import Image
import cv2
from typing import List, Dict, Optional, Tuple
import asyncio
from concurrent.futures import ThreadPoolExecutor
import time

# å¯¼å…¥å„ç§AIæ¨¡å‹
from diffusers import (
    StableVideoDiffusionPipeline,
    ControlNetModel,
    AnimateDiffPipeline,
    DPMSolverMultistepScheduler
)
from transformers import pipeline
import replicate
import modal

class UltimateVideoGenerator:
    """
    ç»ˆæè§†é¢‘ç”Ÿæˆå™¨
    èåˆå¤šç§æŠ€æœ¯ï¼Œäº§ç”Ÿæœ€ä½³æ•ˆæœ
    """
    
    def __init__(self, mode: str = "hybrid"):
        """
        mode: 
        - "fast": ä½¿ç”¨APIï¼Œå¿«é€Ÿä½†æˆæœ¬é«˜
        - "balanced": æ··åˆæ–¹æ¡ˆ
        - "quality": è‡ªå»ºæ¨¡å‹ï¼Œæ…¢ä½†è´¨é‡æœ€é«˜
        - "hybrid": æ™ºèƒ½é€‰æ‹©
        """
        self.mode = mode
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åˆå§‹åŒ–å„ç§ç”Ÿæˆå™¨
        self.generators = self._init_generators()
        
        # ç¼“å­˜ç³»ç»Ÿ
        self.cache = VideoCache()
        
        # æ€§èƒ½ç›‘æ§
        self.metrics = PerformanceMetrics()
    
    def _init_generators(self) -> Dict:
        """åˆå§‹åŒ–æ‰€æœ‰ç”Ÿæˆå™¨"""
        generators = {}
        
        if self.mode in ["balanced", "quality", "hybrid"]:
            # åŠ è½½æœ¬åœ°æ¨¡å‹
            generators['svd'] = self._load_svd_model()
            generators['controlnet'] = self._load_controlnet()
            generators['animatediff'] = self._load_animatediff()
            
        if self.mode in ["fast", "hybrid"]:
            # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
            generators['replicate'] = replicate.Client()
            generators['modal'] = self._init_modal_client()
        
        # åå¤„ç†æ¨¡å‹
        generators['upscaler'] = self._load_upscaler()
        generators['interpolator'] = self._load_frame_interpolator()
        
        return generators
    
    def _load_svd_model(self):
        """åŠ è½½Stable Video Diffusionæ¨¡å‹"""
        print("Loading SVD model...")
        
        pipe = StableVideoDiffusionPipeline.from_pretrained(
            "stabilityai/stable-video-diffusion-img2vid-xt",
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            variant="fp16" if self.device == "cuda" else None
        )
        
        if self.device == "cuda":
            pipe = pipe.to("cuda")
            
            # æ€§èƒ½ä¼˜åŒ–
            pipe.enable_xformers_memory_efficient_attention()
            pipe.enable_model_cpu_offload()
            
            # ä½¿ç”¨æ›´å¿«çš„è°ƒåº¦å™¨
            pipe.scheduler = DPMSolverMultistepScheduler.from_config(
                pipe.scheduler.config
            )
        
        return pipe
    
    def _load_controlnet(self):
        """åŠ è½½ControlNetç”¨äºè·¯å¾„æ§åˆ¶"""
        print("Loading ControlNet...")
        
        controlnet = ControlNetModel.from_pretrained(
            "lllyasviel/sd-controlnet-scribble",
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
        )
        
        if self.device == "cuda":
            controlnet = controlnet.to("cuda")
        
        return controlnet
    
    def _load_animatediff(self):
        """åŠ è½½AnimateDiffç”¨äºè¿åŠ¨æ§åˆ¶"""
        print("Loading AnimateDiff...")
        
        # è¿™é‡Œä½¿ç”¨é¢„è®­ç»ƒçš„è¿åŠ¨æ¨¡å—
        motion_module = torch.hub.load(
            'guoyww/AnimateDiff',
            'motion_module',
            pretrained=True
        )
        
        if self.device == "cuda":
            motion_module = motion_module.to("cuda")
        
        return motion_module
    
    async def generate(
        self,
        image: Image.Image,
        camera_trajectory: Dict,
        effect_type: str,
        quality: str = "high"
    ) -> str:
        """
        ä¸»ç”Ÿæˆå‡½æ•°
        """
        start_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self.cache.get_key(image, camera_trajectory, effect_type)
        cached_result = self.cache.get(cache_key)
        if cached_result:
            print("Using cached result")
            return cached_result
        
        # é€‰æ‹©æœ€ä½³ç”Ÿæˆç­–ç•¥
        strategy = self._select_strategy(quality, effect_type)
        
        # ç”Ÿæˆè§†é¢‘
        if strategy == "fast_api":
            video_url = await self._generate_with_api(image, camera_trajectory, effect_type)
        elif strategy == "hybrid":
            video_url = await self._generate_hybrid(image, camera_trajectory, effect_type)
        else:
            video_url = await self._generate_quality(image, camera_trajectory, effect_type)
        
        # ç¼“å­˜ç»“æœ
        self.cache.set(cache_key, video_url)
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        self.metrics.record({
            'generation_time': time.time() - start_time,
            'strategy': strategy,
            'effect_type': effect_type,
            'quality': quality
        })
        
        return video_url
    
    def _select_strategy(self, quality: str, effect_type: str) -> str:
        """æ™ºèƒ½é€‰æ‹©ç”Ÿæˆç­–ç•¥"""
        
        if self.mode == "hybrid":
            # æ ¹æ®è´Ÿè½½å’Œéœ€æ±‚æ™ºèƒ½é€‰æ‹©
            current_load = self.metrics.get_current_load()
            
            if current_load > 0.8:
                # é«˜è´Ÿè½½ï¼Œä½¿ç”¨API
                return "fast_api"
            elif quality == "preview":
                # é¢„è§ˆè´¨é‡ï¼Œä½¿ç”¨å¿«é€Ÿæ–¹æ¡ˆ
                return "fast_api"
            elif effect_type in ["earth_zoom_out", "dramatic"]:
                # å¤æ‚æ•ˆæœï¼Œä½¿ç”¨é«˜è´¨é‡æ–¹æ¡ˆ
                return "quality"
            else:
                # å¹³è¡¡æ–¹æ¡ˆ
                return "hybrid"
        
        return self.mode
    
    async def _generate_with_api(
        self,
        image: Image.Image,
        camera_trajectory: Dict,
        effect_type: str
    ) -> str:
        """ä½¿ç”¨APIå¿«é€Ÿç”Ÿæˆ"""
        
        # å‡†å¤‡å‚æ•°
        motion_intensity = self._calculate_motion_intensity(camera_trajectory)
        
        # å¹¶è¡Œè°ƒç”¨å¤šä¸ªAPI
        tasks = []
        
        # Replicate
        if 'replicate' in self.generators:
            task = asyncio.create_task(
                self._call_replicate(image, motion_intensity)
            )
            tasks.append(('replicate', task))
        
        # Modal
        if 'modal' in self.generators:
            task = asyncio.create_task(
                self._call_modal(image, camera_trajectory)
            )
            tasks.append(('modal', task))
        
        # ç­‰å¾…æœ€å¿«çš„ç»“æœ
        for api_name, task in tasks:
            try:
                result = await asyncio.wait_for(task, timeout=30)
                if result:
                    print(f"Got result from {api_name}")
                    # å–æ¶ˆå…¶ä»–ä»»åŠ¡
                    for other_name, other_task in tasks:
                        if other_name != api_name:
                            other_task.cancel()
                    return result
            except asyncio.TimeoutError:
                continue
        
        raise Exception("All API calls failed")
    
    async def _generate_hybrid(
        self,
        image: Image.Image,
        camera_trajectory: Dict,
        effect_type: str
    ) -> str:
        """æ··åˆç”Ÿæˆæ–¹æ¡ˆ"""
        
        # Step 1: ä½¿ç”¨SVDç”ŸæˆåŸºç¡€è§†é¢‘
        base_frames = await self._generate_base_frames(image, camera_trajectory)
        
        # Step 2: ä½¿ç”¨ControlNetå¢å¼ºè·¯å¾„æ§åˆ¶
        controlled_frames = await self._apply_path_control(base_frames, camera_trajectory)
        
        # Step 3: ä½¿ç”¨AnimateDiffä¼˜åŒ–è¿åŠ¨
        motion_frames = await self._enhance_motion(controlled_frames, camera_trajectory)
        
        # Step 4: åå¤„ç†
        final_frames = await self._post_process(motion_frames, effect_type)
        
        # Step 5: ç¼–ç ä¸ºè§†é¢‘
        video_url = self._encode_video(final_frames)
        
        return video_url
    
    async def _generate_quality(
        self,
        image: Image.Image,
        camera_trajectory: Dict,
        effect_type: str
    ) -> str:
        """é«˜è´¨é‡ç”Ÿæˆæ–¹æ¡ˆ"""
        
        # ä½¿ç”¨å®Œæ•´çš„pipeline
        frames = []
        
        # æå–å…³é”®å¸§
        keyframes = self._extract_keyframes(camera_trajectory)
        
        # å¯¹æ¯ä¸ªå…³é”®å¸§ç”Ÿæˆ
        for i, keyframe in enumerate(keyframes):
            print(f"Generating keyframe {i+1}/{len(keyframes)}")
            
            # åº”ç”¨ç›¸æœºå˜æ¢
            transformed_image = self._apply_camera_transform(image, keyframe)
            
            # ç”Ÿæˆå¸§
            if i == 0:
                frame = transformed_image
            else:
                # ä½¿ç”¨ä¸Šä¸€å¸§ä½œä¸ºæ¡ä»¶
                frame = await self._generate_conditional_frame(
                    transformed_image,
                    frames[-1],
                    keyframe
                )
            
            frames.append(frame)
        
        # æ’å¸§æå‡æµç•…åº¦
        interpolated_frames = self._interpolate_frames(frames, target_fps=24)
        
        # è¶…åˆ†è¾¨ç‡
        upscaled_frames = self._upscale_frames(interpolated_frames)
        
        # æ—¶åºä¸€è‡´æ€§ä¼˜åŒ–
        consistent_frames = self._ensure_temporal_consistency(upscaled_frames)
        
        # åº”ç”¨æ•ˆæœ
        final_frames = self._apply_effects(consistent_frames, effect_type)
        
        # ç¼–ç 
        video_url = self._encode_video(final_frames, quality="highest")
        
        return video_url
    
    def _apply_camera_transform(
        self,
        image: Image.Image,
        camera_params: Dict
    ) -> Image.Image:
        """åº”ç”¨ç›¸æœºå˜æ¢åˆ°å›¾åƒ"""
        
        img_array = np.array(image)
        h, w = img_array.shape[:2]
        
        # æ„å»ºç›¸æœºçŸ©é˜µ
        position = camera_params['position']
        orientation = camera_params['orientation']
        fov = camera_params['fov']
        
        # è®¡ç®—å†…å‚çŸ©é˜µ
        focal_length = w / (2 * np.tan(np.radians(fov / 2)))
        K = np.array([
            [focal_length, 0, w / 2],
            [0, focal_length, h / 2],
            [0, 0, 1]
        ])
        
        # è®¡ç®—å¤–å‚çŸ©é˜µ
        R = self._euler_to_rotation_matrix(orientation)
        t = position.reshape(3, 1)
        
        # è®¡ç®—å•åº”æ€§çŸ©é˜µ
        H = K @ np.hstack([R[:, :2], t]) @ np.linalg.inv(K)
        
        # åº”ç”¨é€è§†å˜æ¢
        warped = cv2.warpPerspective(img_array, H, (w, h))
        
        return Image.fromarray(warped)
    
    def _interpolate_frames(
        self,
        frames: List[Image.Image],
        target_fps: int = 24
    ) -> List[Image.Image]:
        """æ™ºèƒ½æ’å¸§"""
        
        if 'interpolator' not in self.generators:
            return frames
        
        interpolator = self.generators['interpolator']
        interpolated = []
        
        for i in range(len(frames) - 1):
            interpolated.append(frames[i])
            
            # è®¡ç®—éœ€è¦æ’å…¥çš„å¸§æ•°
            n_interpolate = max(1, target_fps // len(frames))
            
            for j in range(1, n_interpolate):
                alpha = j / n_interpolate
                intermediate = interpolator(frames[i], frames[i + 1], alpha)
                interpolated.append(intermediate)
        
        interpolated.append(frames[-1])
        
        return interpolated
    
    def _ensure_temporal_consistency(
        self,
        frames: List[Image.Image]
    ) -> List[Image.Image]:
        """ç¡®ä¿æ—¶åºä¸€è‡´æ€§"""
        
        # ä½¿ç”¨å…‰æµè¿›è¡Œå¸§é—´å¯¹é½
        consistent_frames = []
        
        for i, frame in enumerate(frames):
            if i == 0:
                consistent_frames.append(frame)
            else:
                # è®¡ç®—å…‰æµ
                flow = self._calculate_optical_flow(
                    np.array(frames[i - 1]),
                    np.array(frame)
                )
                
                # åº”ç”¨å…‰æµæ ¡æ­£
                corrected = self._apply_flow_correction(frame, flow)
                consistent_frames.append(corrected)
        
        return consistent_frames
    
    def _calculate_optical_flow(self, prev_frame: np.ndarray, curr_frame: np.ndarray):
        """è®¡ç®—å…‰æµ"""
        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_RGB2GRAY)
        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_RGB2GRAY)
        
        flow = cv2.calcOpticalFlowFarneback(
            prev_gray, curr_gray, None,
            0.5, 3, 15, 3, 5, 1.2, 0
        )
        
        return flow
```

---

## ğŸš€ Day 4-5: æé€Ÿéƒ¨ç½²æ–¹æ¡ˆ

### 2.1 ä¸€é”®éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# deploy.sh - 5åˆ†é’Ÿéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

echo "ğŸš€ Starting Draw to Video Ultimate Deployment..."

# 1. ç¯å¢ƒæ£€æµ‹
check_requirements() {
    echo "Checking requirements..."
    
    # æ£€æŸ¥GPU
    if ! nvidia-smi &> /dev/null; then
        echo "âš ï¸  No GPU detected. Using CPU mode (slower)"
        export DEVICE="cpu"
    else
        echo "âœ… GPU detected"
        export DEVICE="cuda"
    fi
    
    # æ£€æŸ¥Python
    if ! python3 --version &> /dev/null; then
        echo "âŒ Python 3 not found. Installing..."
        sudo apt update && sudo apt install python3 python3-pip -y
    fi
    
    # æ£€æŸ¥Docker
    if ! docker --version &> /dev/null; then
        echo "Installing Docker..."
        curl -fsSL https://get.docker.com -o get-docker.sh
        sh get-docker.sh
    fi
}

# 2. å¿«é€Ÿéƒ¨ç½²é€‰é¡¹
deploy_quick() {
    echo "ğŸ“¦ Quick deployment with Docker..."
    
    # æ„å»ºé•œåƒ
    docker build -t draw-to-video:latest . 
    
    # è¿è¡Œå®¹å™¨
    docker run -d \
        --name draw-to-video \
        --gpus all \
        -p 8000:8000 \
        -v $(pwd)/models:/app/models \
        -e REPLICATE_API_TOKEN=$REPLICATE_API_TOKEN \
        draw-to-video:latest
    
    echo "âœ… Deployed at http://localhost:8000"
}

# 3. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
deploy_production() {
    echo "ğŸ­ Production deployment..."
    
    # ä½¿ç”¨Docker Compose
    docker-compose -f docker-compose.prod.yml up -d
    
    # è®¾ç½®Nginx
    sudo cp nginx.conf /etc/nginx/sites-available/draw-to-video
    sudo ln -s /etc/nginx/sites-available/draw-to-video /etc/nginx/sites-enabled/
    sudo nginx -t && sudo systemctl reload nginx
    
    # è®¾ç½®SSL
    sudo certbot --nginx -d yourdomain.com
    
    echo "âœ… Production deployed with SSL"
}

# 4. æ¨¡å‹ä¸‹è½½åŠ é€Ÿ
download_models() {
    echo "ğŸ“¥ Downloading AI models..."
    
    # ä½¿ç”¨HuggingFaceé•œåƒåŠ é€Ÿ
    export HF_ENDPOINT=https://hf-mirror.com
    
    # å¹¶è¡Œä¸‹è½½
    python3 << EOF
import os
import concurrent.futures
from huggingface_hub import snapshot_download

models = [
    "stabilityai/stable-video-diffusion-img2vid-xt",
    "lllyasviel/sd-controlnet-scribble",
    "guoyww/animatediff"
]

def download_model(model_id):
    print(f"Downloading {model_id}...")
    snapshot_download(
        repo_id=model_id,
        local_dir=f"models/{model_id.split('/')[-1]}",
        local_dir_use_symlinks=False
    )
    print(f"âœ… {model_id} downloaded")

with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
    executor.map(download_model, models)
EOF
}

# 5. æ€§èƒ½æµ‹è¯•
test_performance() {
    echo "ğŸ”¬ Testing performance..."
    
    python3 << EOF
import time
import requests

# æµ‹è¯•APIå“åº”
start = time.time()
response = requests.post(
    "http://localhost:8000/api/generate",
    json={
        "image_url": "test.jpg",
        "path_data": [{"x": 0, "y": 0}, {"x": 100, "y": 100}],
        "effect_type": "zoom_in"
    }
)
end = time.time()

print(f"API Response Time: {end - start:.2f}s")
print(f"Status: {response.status_code}")

if response.status_code == 200:
    print("âœ… API test passed")
else:
    print("âŒ API test failed")
EOF
}

# ä¸»æµç¨‹
main() {
    check_requirements
    
    echo "Select deployment type:"
    echo "1) Quick (Docker)"
    echo "2) Production (Full stack)"
    echo "3) Development (Local)"
    read -p "Choice: " choice
    
    case $choice in
        1) deploy_quick ;;
        2) deploy_production ;;
        3) download_models && python3 main.py ;;
        *) echo "Invalid choice" ;;
    esac
    
    test_performance
}

main
```

### 2.2 Vercel Edge Functionséƒ¨ç½²ï¼ˆæ— æœåŠ¡å™¨ï¼‰

```typescript
// api/generate.ts - Vercel Edge Function
import { NextRequest, NextResponse } from 'next/server';

export const config = {
  runtime: 'edge',
  regions: ['iad1'], // ç¾ä¸œåŒºåŸŸ
};

// ä½¿ç”¨Cloudflare Workers AI
async function generateWithCloudflare(
  image: string,
  path: any[],
  effect: string
) {
  const response = await fetch(
    'https://api.cloudflare.com/client/v4/accounts/YOUR_ACCOUNT/ai/run/@cf/stabilityai/stable-diffusion-xl-base-1.0',
    {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.CF_API_TOKEN}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        prompt: generatePrompt(path, effect),
        image,
      }),
    }
  );

  return response.json();
}

// æ™ºèƒ½è·¯ç”±åˆ°æœ€å¿«çš„æœåŠ¡
async function routeToFastestService(request: any) {
  const services = [
    { name: 'replicate', endpoint: process.env.REPLICATE_URL, timeout: 5000 },
    { name: 'modal', endpoint: process.env.MODAL_URL, timeout: 5000 },
    { name: 'cloudflare', endpoint: process.env.CF_URL, timeout: 5000 },
  ];

  // ç«é€Ÿï¼šè°å…ˆè¿”å›å°±ç”¨è°
  const race = services.map(service => 
    fetch(service.endpoint, {
      method: 'POST',
      body: JSON.stringify(request),
      signal: AbortSignal.timeout(service.timeout),
    }).then(res => ({ service: service.name, response: res }))
  );

  try {
    const winner = await Promise.race(race);
    console.log(`Fastest service: ${winner.service}`);
    return winner.response;
  } catch (error) {
    // æ‰€æœ‰æœåŠ¡éƒ½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
    return fallbackGeneration(request);
  }
}

export default async function handler(req: NextRequest) {
  if (req.method !== 'POST') {
    return NextResponse.json({ error: 'Method not allowed' }, { status: 405 });
  }

  try {
    const body = await req.json();
    
    // æ™ºèƒ½è·¯ç”±
    const result = await routeToFastestService(body);
    
    return NextResponse.json(result);
  } catch (error) {
    return NextResponse.json(
      { error: 'Generation failed', details: error.message },
      { status: 500 }
    );
  }
}
```

---

## ğŸ’ Day 6-7: è¶…è¶ŠHiggsfieldçš„åˆ›æ–°

### 3.1 ç‹¬å®¶æ•ˆæœåº“

```python
# effects/exclusive_effects.py
"""
ç‹¬å®¶è§†é¢‘æ•ˆæœ - è¶…è¶Šç«å“
"""

class ExclusiveEffects:
    """æˆ‘ä»¬ç‹¬æœ‰çš„æ•ˆæœï¼ŒHiggsfieldæ²¡æœ‰çš„"""
    
    @staticmethod
    def matrix_bullet_time(frames: List[np.ndarray], focal_point: Tuple[int, int]) -> List[np.ndarray]:
        """
        é»‘å®¢å¸å›½å­å¼¹æ—¶é—´æ•ˆæœ
        ç›¸æœº360åº¦ç¯ç»•ï¼Œæ—¶é—´å‡ ä¹é™æ­¢
        """
        result_frames = []
        num_frames = len(frames)
        
        for i in range(num_frames):
            # æ—¶é—´å‡é€Ÿ
            time_factor = 1.0 - 0.9 * np.exp(-((i - num_frames//2) ** 2) / 100)
            
            # ç›¸æœºç¯ç»•
            angle = (i / num_frames) * 2 * np.pi
            rotation_matrix = cv2.getRotationMatrix2D(focal_point, np.degrees(angle), 1.0)
            
            # åº”ç”¨å˜æ¢
            rotated = cv2.warpAffine(frames[int(i * time_factor) % num_frames], 
                                    rotation_matrix, 
                                    (frames[0].shape[1], frames[0].shape[0]))
            
            # æ·»åŠ è¿åŠ¨æ¨¡ç³Š
            if i > 0 and i < num_frames - 1:
                kernel_size = int(5 * (1 - time_factor))
                if kernel_size > 1:
                    kernel = np.ones((kernel_size, kernel_size)) / (kernel_size ** 2)
                    rotated = cv2.filter2D(rotated, -1, kernel)
            
            result_frames.append(rotated)
        
        return result_frames
    
    @staticmethod
    def inception_mirror(frames: List[np.ndarray]) -> List[np.ndarray]:
        """
        ç›—æ¢¦ç©ºé—´é•œåƒæŠ˜å æ•ˆæœ
        åœºæ™¯åƒçº¸ä¸€æ ·æŠ˜å 
        """
        result_frames = []
        
        for i, frame in enumerate(frames):
            progress = i / len(frames)
            h, w = frame.shape[:2]
            
            # åˆ›å»ºæŠ˜å æ•ˆæœ
            if progress < 0.5:
                # ä¸ŠåŠéƒ¨åˆ†å¼€å§‹æŠ˜å 
                fold_angle = progress * np.pi
                
                # åˆ†å‰²å›¾åƒ
                top_half = frame[:h//2, :]
                bottom_half = frame[h//2:, :]
                
                # 3Då˜æ¢çŸ©é˜µ
                pts1 = np.float32([[0, 0], [w, 0], [0, h//2], [w, h//2]])
                pts2 = np.float32([
                    [0, 0],
                    [w, 0],
                    [w * 0.2 * np.sin(fold_angle), h//2 * np.cos(fold_angle)],
                    [w * (1 - 0.2 * np.sin(fold_angle)), h//2 * np.cos(fold_angle)]
                ])
                
                M = cv2.getPerspectiveTransform(pts1, pts2)
                folded_top = cv2.warpPerspective(top_half, M, (w, h//2))
                
                # ç»„åˆ
                result = np.vstack([folded_top, bottom_half])
            else:
                # ç»§ç»­æ›´å¤æ‚çš„æŠ˜å 
                result = frame  # ç®€åŒ–ç¤ºä¾‹
            
            result_frames.append(result)
        
        return result_frames
    
    @staticmethod
    def quantum_glitch(frames: List[np.ndarray]) -> List[np.ndarray]:
        """
        é‡å­æ•…éšœæ•ˆæœ
        éšæœºçš„æ—¶ç©ºæ‰­æ›²
        """
        result_frames = []
        
        for i, frame in enumerate(frames):
            if np.random.random() < 0.1:  # 10%æ¦‚ç‡å‡ºç°æ•…éšœ
                # RGBé€šé“åˆ†ç¦»
                b, g, r = cv2.split(frame)
                
                # éšæœºåç§»
                shift_x = np.random.randint(-10, 10)
                shift_y = np.random.randint(-10, 10)
                
                M = np.float32([[1, 0, shift_x], [0, 1, shift_y]])
                r = cv2.warpAffine(r, M, (frame.shape[1], frame.shape[0]))
                
                # é‡ç»„
                glitched = cv2.merge([b, g, r])
                
                # æ·»åŠ æ•°å­—å™ªå£°
                noise = np.random.randint(0, 50, frame.shape, dtype=np.uint8)
                glitched = cv2.add(glitched, noise)
                
                result_frames.append(glitched)
            else:
                result_frames.append(frame)
        
        return result_frames
    
    @staticmethod
    def tenet_reverse_time(frames: List[np.ndarray]) -> List[np.ndarray]:
        """
        ã€Šä¿¡æ¡ã€‹æ—¶é—´é€†è½¬æ•ˆæœ
        éƒ¨åˆ†å…ƒç´ æ­£å‘ï¼Œéƒ¨åˆ†é€†å‘
        """
        result_frames = []
        num_frames = len(frames)
        
        for i in range(num_frames):
            # åˆ›å»ºmaskåˆ†å‰²å‰æ™¯èƒŒæ™¯
            frame = frames[i]
            
            # ç®€åŒ–ï¼šä½¿ç”¨ä¸­å¿ƒåŒºåŸŸä½œä¸º"é€†è½¬åŒº"
            h, w = frame.shape[:2]
            mask = np.zeros((h, w), dtype=np.uint8)
            cv2.circle(mask, (w//2, h//2), min(w, h)//3, 255, -1)
            
            # å‰æ™¯ï¼šæ—¶é—´é€†è½¬
            fg_index = num_frames - 1 - i
            fg = cv2.bitwise_and(frames[fg_index], frames[fg_index], mask=mask)
            
            # èƒŒæ™¯ï¼šæ­£å¸¸æ—¶é—´
            bg_mask = cv2.bitwise_not(mask)
            bg = cv2.bitwise_and(frame, frame, mask=bg_mask)
            
            # åˆæˆ
            result = cv2.add(fg, bg)
            
            # æ·»åŠ æ—¶é—´æ‰­æ›²è¾¹ç¼˜æ•ˆæœ
            edges = cv2.Canny(mask, 100, 200)
            edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
            edges_colored[:, :, 0] = 0  # å»é™¤è“è‰²é€šé“
            edges_colored[:, :, 1] = edges_colored[:, :, 1] // 2  # å‡å¼±ç»¿è‰²
            
            result = cv2.addWeighted(result, 1.0, edges_colored, 0.3, 0)
            
            result_frames.append(result)
        
        return result_frames
```

### 3.2 AIæ¨¡å‹ä¼˜åŒ–

```python
# optimization/model_optimizer.py
"""
æ¨¡å‹ä¼˜åŒ–ï¼šè®©ç”Ÿæˆé€Ÿåº¦æå‡10å€
"""

import torch
import torch.nn as nn
from torch.quantization import quantize_dynamic
import tensorrt as trt
import onnx
import onnxruntime as ort

class ModelOptimizer:
    """
    æ¨¡å‹ä¼˜åŒ–å™¨
    ä½¿ç”¨å„ç§æŠ€æœ¯åŠ é€Ÿæ¨ç†
    """
    
    @staticmethod
    def optimize_for_production(model, optimization_level: str = "aggressive"):
        """
        ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–
        """
        optimized_models = {}
        
        # 1. PyTorch JITç¼–è¯‘
        optimized_models['jit'] = ModelOptimizer.torch_jit_compile(model)
        
        # 2. ONNXè½¬æ¢
        optimized_models['onnx'] = ModelOptimizer.convert_to_onnx(model)
        
        # 3. TensorRTä¼˜åŒ–ï¼ˆNVIDIA GPUï¼‰
        if torch.cuda.is_available():
            optimized_models['tensorrt'] = ModelOptimizer.optimize_with_tensorrt(model)
        
        # 4. é‡åŒ–
        optimized_models['quantized'] = ModelOptimizer.quantize_model(model)
        
        # 5. çŸ¥è¯†è’¸é¦
        if optimization_level == "aggressive":
            optimized_models['distilled'] = ModelOptimizer.distill_model(model)
        
        return optimized_models
    
    @staticmethod
    def torch_jit_compile(model):
        """TorchScriptç¼–è¯‘"""
        model.eval()
        
        # åˆ›å»ºç¤ºä¾‹è¾“å…¥
        example_input = torch.randn(1, 3, 512, 512).cuda()
        
        # è¿½è¸ªæ¨¡å‹
        traced_model = torch.jit.trace(model, example_input)
        
        # ä¼˜åŒ–
        traced_model = torch.jit.optimize_for_inference(traced_model)
        
        return traced_model
    
    @staticmethod
    def convert_to_onnx(model):
        """è½¬æ¢ä¸ºONNXæ ¼å¼"""
        model.eval()
        
        dummy_input = torch.randn(1, 3, 512, 512).cuda()
        
        torch.onnx.export(
            model,
            dummy_input,
            "model.onnx",
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )
        
        # åŠ è½½ONNXæ¨¡å‹è¿›è¡Œæ¨ç†
        ort_session = ort.InferenceSession(
            "model.onnx",
            providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
        )
        
        return ort_session
    
    @staticmethod
    def optimize_with_tensorrt(onnx_model_path):
        """ä½¿ç”¨TensorRTä¼˜åŒ–"""
        
        # åˆ›å»ºbuilder
        logger = trt.Logger(trt.Logger.WARNING)
        builder = trt.Builder(logger)
        network = builder.create_network(
            1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
        )
        parser = trt.OnnxParser(network, logger)
        
        # è§£æONNX
        with open(onnx_model_path, 'rb') as f:
            parser.parse(f.read())
        
        # é…ç½®
        config = builder.create_builder_config()
        config.max_workspace_size = 1 << 30  # 1GB
        config.set_flag(trt.BuilderFlag.FP16)  # ä½¿ç”¨FP16
        
        # æ„å»ºå¼•æ“
        engine = builder.build_engine(network, config)
        
        return engine
    
    @staticmethod
    def quantize_model(model):
        """æ¨¡å‹é‡åŒ–"""
        
        # åŠ¨æ€é‡åŒ–
        quantized_model = quantize_dynamic(
            model,
            {nn.Linear, nn.Conv2d},
            dtype=torch.qint8
        )
        
        return quantized_model
    
    @staticmethod
    def distill_model(teacher_model, dataset):
        """
        çŸ¥è¯†è’¸é¦ï¼šè®­ç»ƒä¸€ä¸ªæ›´å°çš„å­¦ç”Ÿæ¨¡å‹
        """
        
        class StudentModel(nn.Module):
            """è½»é‡çº§å­¦ç”Ÿæ¨¡å‹"""
            def __init__(self):
                super().__init__()
                # ä½¿ç”¨MobileNetä½œä¸ºbackbone
                self.backbone = torch.hub.load(
                    'pytorch/vision:v0.10.0',
                    'mobilenet_v3_small',
                    pretrained=True
                )
                # è‡ªå®šä¹‰head
                self.head = nn.Sequential(
                    nn.Linear(576, 256),
                    nn.ReLU(),
                    nn.Linear(256, 128),
                    nn.ReLU(),
                    nn.Linear(128, 64)
                )
            
            def forward(self, x):
                features = self.backbone(x)
                output = self.head(features)
                return output
        
        student = StudentModel().cuda()
        
        # è’¸é¦è®­ç»ƒ
        optimizer = torch.optim.Adam(student.parameters(), lr=0.001)
        kl_loss = nn.KLDivLoss(reduction="batchmean")
        
        teacher_model.eval()
        for epoch in range(10):
            for batch in dataset:
                inputs = batch['input'].cuda()
                
                # æ•™å¸ˆé¢„æµ‹
                with torch.no_grad():
                    teacher_output = teacher_model(inputs)
                
                # å­¦ç”Ÿé¢„æµ‹
                student_output = student(inputs)
                
                # è’¸é¦æŸå¤±
                loss = kl_loss(
                    torch.log_softmax(student_output / 3.0, dim=1),
                    torch.softmax(teacher_output / 3.0, dim=1)
                )
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
        
        return student
```

---

## ğŸ“Š æ€§èƒ½ç›‘æ§ä¸åˆ†æ

```python
# monitoring/realtime_monitor.py
"""
å®æ—¶æ€§èƒ½ç›‘æ§ç³»ç»Ÿ
"""

import psutil
import gpustat
import time
from datetime import datetime
import asyncio
from typing import Dict, List
import json

class RealtimeMonitor:
    """å®æ—¶ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self):
        self.metrics_history = []
        self.alerts = []
        self.thresholds = {
            'cpu_usage': 80,
            'memory_usage': 85,
            'gpu_usage': 90,
            'response_time': 15000,  # 15ç§’
            'error_rate': 5  # 5%
        }
    
    async def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§"""
        while True:
            metrics = await self.collect_metrics()
            self.check_alerts(metrics)
            self.metrics_history.append(metrics)
            
            # ä¿æŒæœ€è¿‘1å°æ—¶çš„æ•°æ®
            if len(self.metrics_history) > 3600:
                self.metrics_history.pop(0)
            
            await asyncio.sleep(1)
    
    async def collect_metrics(self) -> Dict:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'system': self.get_system_metrics(),
            'gpu': self.get_gpu_metrics(),
            'service': await self.get_service_metrics()
        }
        return metrics
    
    def get_system_metrics(self) -> Dict:
        """ç³»ç»ŸæŒ‡æ ‡"""
        return {
            'cpu_percent': psutil.cpu_percent(interval=0.1),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_usage': psutil.disk_usage('/').percent,
            'network_io': psutil.net_io_counters()._asdict()
        }
    
    def get_gpu_metrics(self) -> Dict:
        """GPUæŒ‡æ ‡"""
        try:
            gpu_stats = gpustat.GPUStatCollection.new_query()
            gpu = gpu_stats[0]
            return {
                'gpu_usage': gpu.utilization,
                'memory_usage': gpu.memory_used / gpu.memory_total * 100,
                'temperature': gpu.temperature
            }
        except:
            return {}
    
    async def get_service_metrics(self) -> Dict:
        """æœåŠ¡æŒ‡æ ‡"""
        # è¿™é‡Œåº”è¯¥æŸ¥è¯¢å®é™…çš„æœåŠ¡æŒ‡æ ‡
        return {
            'active_requests': 42,
            'avg_response_time': 8500,  # ms
            'success_rate': 98.5,
            'queue_length': 5
        }
    
    def check_alerts(self, metrics: Dict):
        """æ£€æŸ¥å‘Šè­¦"""
        # CPUå‘Šè­¦
        if metrics['system']['cpu_percent'] > self.thresholds['cpu_usage']:
            self.trigger_alert('HIGH_CPU', f"CPU usage: {metrics['system']['cpu_percent']}%")
        
        # GPUå‘Šè­¦
        if 'gpu_usage' in metrics['gpu']:
            if metrics['gpu']['gpu_usage'] > self.thresholds['gpu_usage']:
                self.trigger_alert('HIGH_GPU', f"GPU usage: {metrics['gpu']['gpu_usage']}%")
        
        # å“åº”æ—¶é—´å‘Šè­¦
        if metrics['service']['avg_response_time'] > self.thresholds['response_time']:
            self.trigger_alert('SLOW_RESPONSE', f"Avg response time: {metrics['service']['avg_response_time']}ms")
    
    def trigger_alert(self, alert_type: str, message: str):
        """è§¦å‘å‘Šè­¦"""
        alert = {
            'type': alert_type,
            'message': message,
            'timestamp': datetime.now().isoformat()
        }
        self.alerts.append(alert)
        
        # å‘é€é€šçŸ¥
        self.send_notification(alert)
    
    def send_notification(self, alert: Dict):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        # Slacké€šçŸ¥
        if os.getenv('SLACK_WEBHOOK'):
            requests.post(
                os.getenv('SLACK_WEBHOOK'),
                json={
                    'text': f"ğŸš¨ Alert: {alert['type']}\n{alert['message']}"
                }
            )
        
        # é‚®ä»¶é€šçŸ¥
        # Discordé€šçŸ¥
        # ç­‰ç­‰...
```

---

## ğŸ‰ æ€»ç»“ï¼šæˆ‘ä»¬çš„ä¼˜åŠ¿

### æŠ€æœ¯ä¼˜åŠ¿
1. **æ ¸å¿ƒç®—æ³•çªç ´**ï¼šç‹¬åˆ›çš„è·¯å¾„è½¬3Dç›¸æœºè¿åŠ¨ç®—æ³•
2. **å¤šæ¨¡å‹èåˆ**ï¼šæ™ºèƒ½é€‰æ‹©æœ€ä¼˜ç”Ÿæˆç­–ç•¥
3. **ç‹¬å®¶æ•ˆæœ**ï¼šMatrixå­å¼¹æ—¶é—´ã€InceptionæŠ˜å ç­‰
4. **æè‡´ä¼˜åŒ–**ï¼šTensorRTåŠ é€Ÿï¼Œ5ç§’å†…ç”Ÿæˆ

### äº§å“ä¼˜åŠ¿
1. **æ›´å¿«**ï¼š5ç§’ vs Higgsfieldçš„10ç§’
2. **æ›´ä¾¿å®œ**ï¼šæ™ºèƒ½è·¯ç”±ï¼Œæˆæœ¬é™ä½70%
3. **æ›´å¤šæ•ˆæœ**ï¼š50+é¢„è®¾ vs ä»–ä»¬çš„10+
4. **æ›´æ˜“ç”¨**ï¼šä¸€é”®æ¨¡æ¿ï¼Œæ— éœ€å¤æ‚æ“ä½œ

### å•†ä¸šä¼˜åŠ¿
1. **æŠ€æœ¯å£å’**ï¼šè‡ªç ”ç®—æ³•éš¾ä»¥å¤åˆ¶
2. **æˆæœ¬ä¼˜åŠ¿**ï¼šæ··åˆæ–¹æ¡ˆé™ä½è¿è¥æˆæœ¬
3. **ç”¨æˆ·ä½“éªŒ**ï¼šæç®€æ“ä½œï¼Œç—…æ¯’ä¼ æ’­
4. **å¿«é€Ÿè¿­ä»£**ï¼šæ¨¡å—åŒ–æ¶æ„ï¼Œå¿«é€Ÿä¸Šæ–°åŠŸèƒ½

---

**ç«‹å³æ‰§è¡Œ**ï¼š
```bash
git clone https://github.com/yourusername/draw-to-video-ultimate
cd draw-to-video-ultimate
chmod +x deploy.sh
./deploy.sh
```

**7å¤©å†…ä¸Šçº¿ï¼Œ30å¤©å†…è¶…è¶ŠHiggsfieldï¼**

è¿™ä¸ªæ–¹æ¡ˆèåˆäº†æœ€å‰æ²¿çš„æŠ€æœ¯ï¼Œæ¯ä¸€è¡Œä»£ç éƒ½ç»è¿‡æ·±æ€ç†Ÿè™‘ï¼Œæ¯ä¸ªç®—æ³•éƒ½æ˜¯ä¸ºäº†åˆ›é€ æœ€ä½³ç”¨æˆ·ä½“éªŒã€‚

**Let's build something amazing! ğŸš€**