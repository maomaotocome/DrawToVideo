# Draw to Video æŠ€æœ¯å®ç°è·¯çº¿å›¾ - å®Œæ•´å®æ“æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [æ ¸å¿ƒæŠ€æœ¯æ¶æ„](#æ ¸å¿ƒæŠ€æœ¯æ¶æ„)
2. [Phase 1: MVPå¿«é€Ÿå®ç°ï¼ˆç¬¬1-2å‘¨ï¼‰](#phase-1-mvpå¿«é€Ÿå®ç°ç¬¬1-2å‘¨)
3. [Phase 2: æ•ˆæœä¼˜åŒ–ï¼ˆç¬¬3-4å‘¨ï¼‰](#phase-2-æ•ˆæœä¼˜åŒ–ç¬¬3-4å‘¨)
4. [Phase 3: æˆæœ¬ä¼˜åŒ–ï¼ˆç¬¬2ä¸ªæœˆï¼‰](#phase-3-æˆæœ¬ä¼˜åŒ–ç¬¬2ä¸ªæœˆ)
5. [Phase 4: è‡ªä¸»æŠ€æœ¯ï¼ˆç¬¬3-6ä¸ªæœˆï¼‰](#phase-4-è‡ªä¸»æŠ€æœ¯ç¬¬3-6ä¸ªæœˆ)
6. [è¯¦ç»†ä»£ç å®ç°](#è¯¦ç»†ä»£ç å®ç°)
7. [éƒ¨ç½²ä¸è¿ç»´](#éƒ¨ç½²ä¸è¿ç»´)
8. [æˆæœ¬ç›‘æ§](#æˆæœ¬ç›‘æ§)

---

## ğŸ—ï¸ æ ¸å¿ƒæŠ€æœ¯æ¶æ„

```mermaid
graph TB
    subgraph "å‰ç«¯"
        A[ç”¨æˆ·ç»˜åˆ¶è·¯å¾„] --> B[Canvaså¤„ç†]
        B --> C[è·¯å¾„æ•°æ®æå–]
    end
    
    subgraph "åç«¯å¤„ç†"
        C --> D[è·¯å¾„é¢„å¤„ç†]
        D --> E[è¿åŠ¨å‚æ•°è®¡ç®—]
        E --> F[ä»»åŠ¡é˜Ÿåˆ—]
    end
    
    subgraph "AIç”Ÿæˆ"
        F --> G{ç”Ÿæˆç­–ç•¥}
        G -->|MVP| H[Replicate API]
        G -->|ä¼˜åŒ–| I[æ··åˆæ¨¡å‹]
        G -->|è‡ªä¸»| J[è‡ªå»ºPipeline]
    end
    
    subgraph "åå¤„ç†"
        H --> K[è§†é¢‘ä¼˜åŒ–]
        I --> K
        J --> K
        K --> L[CDNåˆ†å‘]
    end
```

---

## ğŸ“… Phase 1: MVPå¿«é€Ÿå®ç°ï¼ˆç¬¬1-2å‘¨ï¼‰

### Week 1: åŸºç¡€æ¶æ„æ­å»º

#### Day 1-2: ç¯å¢ƒå‡†å¤‡

```bash
# 1. åˆ›å»ºé¡¹ç›®ç»“æ„
mkdir -p draw-to-video/{frontend,backend,services,scripts,configs}
cd draw-to-video

# 2. åˆå§‹åŒ–åç«¯
cd backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. å®‰è£…æ ¸å¿ƒä¾èµ–
cat > requirements.txt << 'EOF'
fastapi==0.104.1
uvicorn==0.24.0
celery==5.3.4
redis==5.0.1
replicate==0.22.0
opencv-python==4.8.1.78
pillow==10.1.0
numpy==1.24.3
boto3==1.29.7
python-multipart==0.0.6
pydantic==2.5.0
python-dotenv==1.0.0
EOF

pip install -r requirements.txt

# 4. é…ç½®ç¯å¢ƒå˜é‡
cat > .env << 'EOF'
REPLICATE_API_TOKEN=r8_xxxxxxxxxxxxx
REDIS_URL=redis://localhost:6379/0
AWS_ACCESS_KEY_ID=xxx
AWS_SECRET_ACCESS_KEY=xxx
S3_BUCKET_NAME=draw-to-video
CLOUDFLARE_ACCOUNT_ID=xxx
CLOUDFLARE_R2_ACCESS_KEY=xxx
CLOUDFLARE_R2_SECRET_KEY=xxx
EOF
```

#### Day 3-4: æ ¸å¿ƒAPIå®ç°

```python
# backend/main.py
from fastapi import FastAPI, UploadFile, File, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any
import uuid
import json
from services.video_generator import VideoGenerator
from services.storage import StorageService
from services.queue_manager import QueueManager

app = FastAPI()

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–æœåŠ¡
video_gen = VideoGenerator()
storage = StorageService()
queue = QueueManager()

class GenerateRequest(BaseModel):
    image_url: str
    path_data: List[Dict[str, float]]
    effect_type: str = "zoom_in"
    duration: int = 5
    
class GenerateResponse(BaseModel):
    task_id: str
    status: str
    eta_seconds: int

@app.post("/api/generate", response_model=GenerateResponse)
async def generate_video(
    request: GenerateRequest,
    background_tasks: BackgroundTasks
):
    """ç”Ÿæˆè§†é¢‘çš„ä¸»API"""
    task_id = str(uuid.uuid4())
    
    # æ·»åŠ åˆ°é˜Ÿåˆ—
    task_data = {
        "task_id": task_id,
        "image_url": request.image_url,
        "path_data": request.path_data,
        "effect_type": request.effect_type,
        "duration": request.duration
    }
    
    # å¼‚æ­¥å¤„ç†
    background_tasks.add_task(
        process_video_generation,
        task_data
    )
    
    # ç«‹å³è¿”å›ä»»åŠ¡ID
    return GenerateResponse(
        task_id=task_id,
        status="processing",
        eta_seconds=10
    )

async def process_video_generation(task_data: dict):
    """å¼‚æ­¥å¤„ç†è§†é¢‘ç”Ÿæˆ"""
    try:
        # æ›´æ–°çŠ¶æ€
        queue.update_status(task_data["task_id"], "processing")
        
        # ç”Ÿæˆè§†é¢‘
        video_url = await video_gen.generate(
            image_url=task_data["image_url"],
            path_data=task_data["path_data"],
            effect_type=task_data["effect_type"]
        )
        
        # ä¸Šä¼ åˆ°å­˜å‚¨
        final_url = await storage.upload_video(video_url, task_data["task_id"])
        
        # æ›´æ–°å®ŒæˆçŠ¶æ€
        queue.update_status(
            task_data["task_id"], 
            "completed",
            {"video_url": final_url}
        )
        
    except Exception as e:
        queue.update_status(
            task_data["task_id"],
            "failed",
            {"error": str(e)}
        )

@app.get("/api/status/{task_id}")
async def get_status(task_id: str):
    """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"""
    status = queue.get_status(task_id)
    return status

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### Day 5: è§†é¢‘ç”ŸæˆæœåŠ¡å®ç°

```python
# services/video_generator.py
import replicate
import os
import numpy as np
from PIL import Image
import cv2
import tempfile
from typing import List, Dict
import asyncio
import aiohttp

class VideoGenerator:
    def __init__(self):
        self.replicate_token = os.getenv("REPLICATE_API_TOKEN")
        os.environ["REPLICATE_API_TOKEN"] = self.replicate_token
        
    async def generate(
        self, 
        image_url: str, 
        path_data: List[Dict[str, float]], 
        effect_type: str
    ) -> str:
        """
        ä¸»ç”Ÿæˆå‡½æ•° - MVPç‰ˆæœ¬ä½¿ç”¨Replicate API
        """
        # Step 1: ä¸‹è½½å¹¶å¤„ç†å›¾ç‰‡
        image = await self.download_image(image_url)
        
        # Step 2: å°†è·¯å¾„è½¬æ¢ä¸ºè¿åŠ¨å‚æ•°
        motion_params = self.path_to_motion_params(path_data, effect_type)
        
        # Step 3: ç”Ÿæˆè§†é¢‘ï¼ˆä½¿ç”¨Replicateï¼‰
        video_url = await self.generate_with_replicate(image, motion_params)
        
        return video_url
    
    async def download_image(self, url: str) -> Image.Image:
        """ä¸‹è½½å›¾ç‰‡"""
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                image_data = await response.read()
                image = Image.open(io.BytesIO(image_data))
                return image
    
    def path_to_motion_params(
        self, 
        path_data: List[Dict[str, float]], 
        effect_type: str
    ) -> Dict:
        """
        å°†ç”¨æˆ·ç»˜åˆ¶çš„è·¯å¾„è½¬æ¢ä¸ºè¿åŠ¨å‚æ•°
        è¿™æ˜¯æ ¸å¿ƒç®—æ³•
        """
        params = {
            "effect_type": effect_type,
            "keyframes": [],
            "camera_path": []
        }
        
        # é‡‡æ ·å…³é”®ç‚¹ï¼ˆæ¯10ä¸ªç‚¹å–ä¸€ä¸ªï¼‰
        sampled_points = path_data[::max(1, len(path_data)//10)]
        
        for i, point in enumerate(sampled_points):
            progress = i / len(sampled_points)
            
            if effect_type == "zoom_in":
                params["keyframes"].append({
                    "time": progress,
                    "zoom": 1.0 + progress * 2.0,  # é€æ¸æ”¾å¤§3å€
                    "x": point["x"],
                    "y": point["y"],
                    "rotation": 0
                })
                
            elif effect_type == "orbit":
                angle = progress * 360  # å®Œæ•´æ—‹è½¬
                params["keyframes"].append({
                    "time": progress,
                    "zoom": 1.2,
                    "x": point["x"] + np.cos(np.radians(angle)) * 50,
                    "y": point["y"] + np.sin(np.radians(angle)) * 50,
                    "rotation": angle
                })
                
            elif effect_type == "pull_back":
                params["keyframes"].append({
                    "time": progress,
                    "zoom": 3.0 - progress * 2.0,  # ä»3å€ç¼©å°åˆ°1å€
                    "x": point["x"],
                    "y": point["y"],
                    "rotation": 0
                })
        
        return params
    
    async def generate_with_replicate(
        self, 
        image: Image.Image, 
        motion_params: Dict
    ) -> str:
        """
        ä½¿ç”¨Replicate APIç”Ÿæˆè§†é¢‘
        MVPé˜¶æ®µä½¿ç”¨stable-video-diffusion
        """
        # ä¿å­˜å›¾ç‰‡åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp:
            image.save(tmp.name)
            tmp_path = tmp.name
        
        try:
            # è°ƒç”¨Replicate API
            output = replicate.run(
                "stability-ai/stable-video-diffusion:3f0457e4619daac51203dedb472816fd4af51f3149fa7a9e0b5ffcf1b8172438",
                input={
                    "input_image": open(tmp_path, "rb"),
                    "video_length": "25_frames_with_svd_xt",
                    "sizing_strategy": "maintain_aspect_ratio",
                    "frames_per_second": 24,
                    "motion_bucket_id": self.calculate_motion_intensity(motion_params),
                    "cond_aug": 0.02,
                    "decoding_t": 7,
                    "seed": 42
                }
            )
            
            # è¿”å›ç”Ÿæˆçš„è§†é¢‘URL
            return output
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(tmp_path)
    
    def calculate_motion_intensity(self, motion_params: Dict) -> int:
        """
        æ ¹æ®è¿åŠ¨å‚æ•°è®¡ç®—motion_bucket_id
        èŒƒå›´ï¼š1-255ï¼Œå€¼è¶Šå¤§è¿åŠ¨è¶Šå¼º
        """
        effect_type = motion_params.get("effect_type", "zoom_in")
        
        intensity_map = {
            "zoom_in": 127,      # ä¸­ç­‰è¿åŠ¨
            "orbit": 180,        # è¾ƒå¼ºè¿åŠ¨
            "pull_back": 100,    # è¾ƒå¼±è¿åŠ¨
            "dramatic": 220,     # å¼ºçƒˆè¿åŠ¨
            "subtle": 50         # å¾®å¦™è¿åŠ¨
        }
        
        return intensity_map.get(effect_type, 127)
```

#### Day 6-7: å‰ç«¯é›†æˆ

```typescript
// frontend/components/VideoGenerator.tsx
import React, { useState, useRef } from 'react';
import axios from 'axios';

const API_URL = process.env.NEXT_PUBLIC_API_URL || 'http://localhost:8000';

export const VideoGenerator: React.FC = () => {
  const [isGenerating, setIsGenerating] = useState(false);
  const [videoUrl, setVideoUrl] = useState<string | null>(null);
  const [taskId, setTaskId] = useState<string | null>(null);
  const [progress, setProgress] = useState(0);
  
  const generateVideo = async (pathData: any[], imageUrl: string, effect: string) => {
    setIsGenerating(true);
    setProgress(0);
    
    try {
      // 1. å‘èµ·ç”Ÿæˆè¯·æ±‚
      const response = await axios.post(`${API_URL}/api/generate`, {
        image_url: imageUrl,
        path_data: pathData,
        effect_type: effect,
        duration: 5
      });
      
      const { task_id } = response.data;
      setTaskId(task_id);
      
      // 2. è½®è¯¢çŠ¶æ€
      const pollInterval = setInterval(async () => {
        const statusResponse = await axios.get(`${API_URL}/api/status/${task_id}`);
        const { status, result, progress: currentProgress } = statusResponse.data;
        
        setProgress(currentProgress || 50);
        
        if (status === 'completed') {
          clearInterval(pollInterval);
          setVideoUrl(result.video_url);
          setIsGenerating(false);
          setProgress(100);
        } else if (status === 'failed') {
          clearInterval(pollInterval);
          setIsGenerating(false);
          alert('Generation failed: ' + result.error);
        }
      }, 1000);
      
    } catch (error) {
      console.error('Generation error:', error);
      setIsGenerating(false);
    }
  };
  
  return (
    <div className="video-generator">
      {/* UIç»„ä»¶ */}
      {isGenerating && (
        <div className="progress-bar">
          <div 
            className="progress-fill" 
            style={{ width: `${progress}%` }}
          />
          <span>{progress}% - Generating magic...</span>
        </div>
      )}
      
      {videoUrl && (
        <video 
          src={videoUrl} 
          controls 
          autoPlay 
          loop 
          className="generated-video"
        />
      )}
    </div>
  );
};
```

### Week 2: ä¼˜åŒ–ä¸æµ‹è¯•

#### Day 8-9: æ·»åŠ æ›´å¤šæ•ˆæœ

```python
# services/effects_library.py
class EffectsLibrary:
    """é¢„è®¾æ•ˆæœåº“"""
    
    @staticmethod
    def get_effect_params(effect_name: str) -> Dict:
        """è·å–é¢„è®¾æ•ˆæœå‚æ•°"""
        
        effects = {
            "earth_zoom_out": {
                "name": "Earth Zoom Out",
                "description": "TikTok viral effect",
                "motion_bucket_id": 200,
                "keyframe_pattern": "exponential_zoom_out",
                "duration": 5,
                "fps": 24
            },
            "eye_zoom": {
                "name": "Eye Zoom",
                "description": "Dramatic eye zoom effect",
                "motion_bucket_id": 150,
                "keyframe_pattern": "linear_zoom_in",
                "focus_point": "center",
                "duration": 3
            },
            "product_orbit": {
                "name": "Product 360",
                "description": "360 degree product showcase",
                "motion_bucket_id": 180,
                "keyframe_pattern": "circular_orbit",
                "revolutions": 1,
                "duration": 6
            },
            "parallax_slide": {
                "name": "Parallax Slide",
                "description": "Multi-layer parallax effect",
                "motion_bucket_id": 120,
                "keyframe_pattern": "horizontal_slide",
                "layers": 3,
                "duration": 4
            },
            "vertigo_effect": {
                "name": "Vertigo",
                "description": "Hitchcock zoom effect",
                "motion_bucket_id": 160,
                "keyframe_pattern": "dolly_zoom",
                "duration": 4
            }
        }
        
        return effects.get(effect_name, effects["earth_zoom_out"])
    
    @staticmethod
    def generate_keyframes(effect_name: str, path_data: List) -> List[Dict]:
        """æ ¹æ®æ•ˆæœç”Ÿæˆå…³é”®å¸§"""
        
        effect = EffectsLibrary.get_effect_params(effect_name)
        pattern = effect["keyframe_pattern"]
        
        if pattern == "exponential_zoom_out":
            return EffectsLibrary._exponential_zoom_out(path_data)
        elif pattern == "linear_zoom_in":
            return EffectsLibrary._linear_zoom_in(path_data)
        elif pattern == "circular_orbit":
            return EffectsLibrary._circular_orbit(path_data)
        # ... æ›´å¤šæ¨¡å¼
    
    @staticmethod
    def _exponential_zoom_out(path_data: List) -> List[Dict]:
        """æŒ‡æ•°çº§æ‹‰è¿œæ•ˆæœ"""
        keyframes = []
        for i, point in enumerate(path_data[::5]):
            t = i / (len(path_data) // 5)
            zoom = 1.0 * (10 ** (t * 2))  # æŒ‡æ•°å¢é•¿åˆ°100å€
            keyframes.append({
                "time": t,
                "zoom": zoom,
                "x": point["x"],
                "y": point["y"],
                "rotation": 0,
                "easing": "ease-out"
            })
        return keyframes
```

#### Day 10-11: æ€§èƒ½ä¼˜åŒ–

```python
# services/cache_manager.py
import redis
import hashlib
import json
from typing import Optional

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ - é¿å…é‡å¤ç”Ÿæˆ"""
    
    def __init__(self):
        self.redis_client = redis.from_url(os.getenv("REDIS_URL"))
        self.cache_ttl = 86400  # 24å°æ—¶
    
    def get_cache_key(self, image_url: str, path_data: List, effect: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        data_str = f"{image_url}:{json.dumps(path_data)}:{effect}"
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def get_cached_video(self, image_url: str, path_data: List, effect: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„è§†é¢‘"""
        key = self.get_cache_key(image_url, path_data, effect)
        cached = self.redis_client.get(f"video:{key}")
        
        if cached:
            return cached.decode('utf-8')
        return None
    
    def cache_video(self, image_url: str, path_data: List, effect: str, video_url: str):
        """ç¼“å­˜ç”Ÿæˆçš„è§†é¢‘"""
        key = self.get_cache_key(image_url, path_data, effect)
        self.redis_client.setex(
            f"video:{key}",
            self.cache_ttl,
            video_url
        )

# åœ¨VideoGeneratorä¸­ä½¿ç”¨ç¼“å­˜
class VideoGenerator:
    def __init__(self):
        self.cache = CacheManager()
        # ... å…¶ä»–åˆå§‹åŒ–
    
    async def generate(self, image_url: str, path_data: List, effect_type: str) -> str:
        # å…ˆæ£€æŸ¥ç¼“å­˜
        cached = self.cache.get_cached_video(image_url, path_data, effect_type)
        if cached:
            return cached
        
        # ç”Ÿæˆæ–°è§†é¢‘
        video_url = await self._generate_new(image_url, path_data, effect_type)
        
        # ç¼“å­˜ç»“æœ
        self.cache.cache_video(image_url, path_data, effect_type, video_url)
        
        return video_url
```

#### Day 12-14: æµ‹è¯•ä¸éƒ¨ç½²

```yaml
# docker-compose.yml
version: '3.8'

services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - REPLICATE_API_TOKEN=${REPLICATE_API_TOKEN}
      - REDIS_URL=redis://redis:6379/0
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
    depends_on:
      - redis
    volumes:
      - ./backend:/app
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - backend

volumes:
  redis_data:
```

---

## ğŸš€ Phase 2: æ•ˆæœä¼˜åŒ–ï¼ˆç¬¬3-4å‘¨ï¼‰

### Week 3: æ··åˆæ¨¡å‹æ–¹æ¡ˆ

#### å®ç°å¤šæ¨¡å‹ç»„åˆ

```python
# services/hybrid_generator.py
import asyncio
from typing import List, Dict
import numpy as np

class HybridVideoGenerator:
    """æ··åˆå¤šä¸ªAIæ¨¡å‹ä»¥è·å¾—æ›´å¥½æ•ˆæœ"""
    
    def __init__(self):
        self.models = {
            "base": "stability-ai/stable-video-diffusion",
            "motion": "lucataco/animate-diff",
            "upscale": "nightmareai/real-esrgan"
        }
    
    async def generate_advanced(
        self,
        image: Image.Image,
        path_data: List[Dict],
        effect_type: str
    ) -> str:
        """
        é«˜çº§ç”Ÿæˆæµç¨‹ï¼š
        1. ä½¿ç”¨SDç”ŸæˆåŸºç¡€è§†é¢‘
        2. ä½¿ç”¨AnimateDiffå¢å¼ºè¿åŠ¨
        3. ä½¿ç”¨ESRGANæå‡è´¨é‡
        """
        
        # Step 1: ç”Ÿæˆå¤šä¸ªå˜ä½“
        variants = await self.generate_variants(image, path_data)
        
        # Step 2: æ™ºèƒ½æ··åˆ
        blended = await self.blend_variants(variants)
        
        # Step 3: åå¤„ç†å¢å¼º
        enhanced = await self.enhance_video(blended)
        
        return enhanced
    
    async def generate_variants(self, image, path_data):
        """å¹¶è¡Œç”Ÿæˆå¤šä¸ªå˜ä½“"""
        tasks = []
        
        # ä¸åŒå¼ºåº¦çš„è¿åŠ¨
        for intensity in [100, 150, 200]:
            task = self.generate_single_variant(image, path_data, intensity)
            tasks.append(task)
        
        variants = await asyncio.gather(*tasks)
        return variants
    
    async def blend_variants(self, variants: List) -> str:
        """æ™ºèƒ½æ··åˆå¤šä¸ªå˜ä½“"""
        # ä½¿ç”¨OpenCVæ··åˆè§†é¢‘
        import cv2
        
        # è¯»å–æ‰€æœ‰è§†é¢‘å¸§
        all_frames = []
        for variant_url in variants:
            frames = self.extract_frames(variant_url)
            all_frames.append(frames)
        
        # æ™ºèƒ½æ··åˆç®—æ³•
        blended_frames = []
        for i in range(len(all_frames[0])):
            # è·å–æ¯ä¸ªå˜ä½“çš„ç¬¬iå¸§
            frames_at_i = [frames[i] for frames in all_frames]
            
            # åŠ æƒå¹³å‡ï¼ˆå¯ä»¥æ ¹æ®è´¨é‡è¯„åˆ†è°ƒæ•´æƒé‡ï¼‰
            weights = [0.5, 0.3, 0.2]  # ä¸»å˜ä½“æƒé‡æ›´é«˜
            blended = np.average(frames_at_i, axis=0, weights=weights)
            
            blended_frames.append(blended.astype(np.uint8))
        
        # ç¼–ç ä¸ºè§†é¢‘
        output_path = self.frames_to_video(blended_frames)
        return output_path
```

### Week 4: è‡ªå®šä¹‰è¿åŠ¨æ§åˆ¶

#### å®ç°ç²¾ç¡®çš„ç›¸æœºè¿åŠ¨æ§åˆ¶

```python
# services/camera_controller.py
import numpy as np
from scipy import interpolate
from typing import List, Tuple

class CameraController:
    """ç²¾ç¡®æ§åˆ¶ç›¸æœºè¿åŠ¨"""
    
    def __init__(self):
        self.fps = 24
        self.resolution = (1920, 1080)
    
    def path_to_camera_trajectory(
        self,
        path_points: List[Dict],
        duration: float,
        effect_type: str
    ) -> List[Dict]:
        """
        å°†2Dè·¯å¾„è½¬æ¢ä¸º3Dç›¸æœºè½¨è¿¹
        è¿”å›æ¯ä¸€å¸§çš„ç›¸æœºå‚æ•°
        """
        
        total_frames = int(self.fps * duration)
        
        # æå–x, yåæ ‡
        x_coords = [p['x'] for p in path_points]
        y_coords = [p['y'] for p in path_points]
        
        # åˆ›å»ºå¹³æ»‘æ’å€¼
        t = np.linspace(0, 1, len(path_points))
        t_new = np.linspace(0, 1, total_frames)
        
        # ä½¿ç”¨ä¸‰æ¬¡æ ·æ¡æ’å€¼
        fx = interpolate.interp1d(t, x_coords, kind='cubic')
        fy = interpolate.interp1d(t, y_coords, kind='cubic')
        
        # ç”Ÿæˆå¹³æ»‘è·¯å¾„
        smooth_x = fx(t_new)
        smooth_y = fy(t_new)
        
        # æ ¹æ®æ•ˆæœç±»å‹ç”Ÿæˆç›¸æœºå‚æ•°
        camera_params = []
        for i in range(total_frames):
            progress = i / total_frames
            
            param = self.calculate_camera_param(
                smooth_x[i],
                smooth_y[i],
                progress,
                effect_type
            )
            
            camera_params.append(param)
        
        return camera_params
    
    def calculate_camera_param(
        self,
        x: float,
        y: float,
        progress: float,
        effect_type: str
    ) -> Dict:
        """è®¡ç®—å•å¸§çš„ç›¸æœºå‚æ•°"""
        
        if effect_type == "zoom_in":
            return {
                "position": [x, y, -100 + progress * 90],  # Zè½´æ¨è¿›
                "target": [x, y, 0],  # çœ‹å‘å½“å‰ç‚¹
                "up": [0, 1, 0],
                "fov": 60 - progress * 30,  # FOVç¼©å°å®ç°zoom
                "roll": 0
            }
            
        elif effect_type == "orbit":
            angle = progress * 2 * np.pi
            radius = 100
            return {
                "position": [
                    x + radius * np.cos(angle),
                    y,
                    radius * np.sin(angle)
                ],
                "target": [x, y, 0],
                "up": [0, 1, 0],
                "fov": 45,
                "roll": 0
            }
            
        elif effect_type == "dolly_zoom":
            # Vertigoæ•ˆæœ
            zoom = 1 + progress * 2
            return {
                "position": [x, y, -100 * zoom],
                "target": [x, y, 0],
                "up": [0, 1, 0],
                "fov": 60 / zoom,  # åå‘è°ƒæ•´FOV
                "roll": 0
            }
        
        # é»˜è®¤å‚æ•°
        return {
            "position": [x, y, -100],
            "target": [0, 0, 0],
            "up": [0, 1, 0],
            "fov": 60,
            "roll": 0
        }
    
    def apply_camera_to_image(
        self,
        image: np.ndarray,
        camera_param: Dict
    ) -> np.ndarray:
        """
        å°†ç›¸æœºå‚æ•°åº”ç”¨åˆ°å›¾åƒ
        ä½¿ç”¨é€è§†å˜æ¢æ¨¡æ‹Ÿç›¸æœºè¿åŠ¨
        """
        
        h, w = image.shape[:2]
        
        # è®¡ç®—é€è§†å˜æ¢çŸ©é˜µ
        src_points = np.float32([[0, 0], [w, 0], [w, h], [0, h]])
        
        # æ ¹æ®ç›¸æœºå‚æ•°è®¡ç®—ç›®æ ‡ç‚¹
        zoom = camera_param.get('fov', 60) / 60
        pos = camera_param['position']
        
        # ç®€åŒ–çš„é€è§†å˜æ¢
        offset_x = pos[0] * 2
        offset_y = pos[1] * 2
        scale = 1 / (1 + pos[2] / 100)
        
        dst_points = np.float32([
            [offset_x, offset_y],
            [w * scale + offset_x, offset_y * scale],
            [w * scale + offset_x, h * scale + offset_y],
            [offset_x, h * scale + offset_y]
        ])
        
        # è®¡ç®—é€è§†å˜æ¢çŸ©é˜µ
        M = cv2.getPerspectiveTransform(src_points, dst_points)
        
        # åº”ç”¨å˜æ¢
        warped = cv2.warpPerspective(image, M, (w, h))
        
        return warped
```

---

## ğŸ’° Phase 3: æˆæœ¬ä¼˜åŒ–ï¼ˆç¬¬2ä¸ªæœˆï¼‰

### è‡ªå»ºæ¨ç†æœåŠ¡

#### ä½¿ç”¨Modal.coméƒ¨ç½²ï¼ˆServerless GPUï¼‰

```python
# deploy/modal_deployment.py
import modal

stub = modal.Stub("draw-to-video")

# å®šä¹‰GPUç¯å¢ƒ
gpu_image = (
    modal.Image.debian_slim()
    .pip_install(
        "torch==2.0.1",
        "transformers==4.35.0",
        "diffusers==0.24.0",
        "accelerate==0.24.0",
        "opencv-python==4.8.1.78",
        "pillow==10.1.0"
    )
    .run_commands("apt-get update && apt-get install -y ffmpeg")
)

@stub.function(
    image=gpu_image,
    gpu="T4",  # ä½¿ç”¨T4 GPUï¼Œæˆæœ¬è¾ƒä½
    timeout=300,
    concurrency_limit=10
)
def generate_video_modal(image_data: bytes, motion_params: dict) -> bytes:
    """åœ¨Modalä¸Šè¿è¡Œçš„è§†é¢‘ç”Ÿæˆå‡½æ•°"""
    
    import torch
    from diffusers import StableVideoDiffusionPipeline
    from PIL import Image
    import io
    
    # åŠ è½½æ¨¡å‹ï¼ˆä¼šè¢«ç¼“å­˜ï¼‰
    pipe = StableVideoDiffusionPipeline.from_pretrained(
        "stabilityai/stable-video-diffusion-img2vid-xt",
        torch_dtype=torch.float16,
        variant="fp16"
    )
    pipe = pipe.to("cuda")
    
    # å¤„ç†è¾“å…¥å›¾åƒ
    image = Image.open(io.BytesIO(image_data))
    
    # ç”Ÿæˆè§†é¢‘
    frames = pipe(
        image,
        num_frames=25,
        decode_chunk_size=8,
        motion_bucket_id=motion_params.get("intensity", 127)
    ).frames
    
    # è½¬æ¢ä¸ºè§†é¢‘å­—èŠ‚
    video_bytes = frames_to_video_bytes(frames)
    
    return video_bytes

@stub.local_entrypoint()
def main():
    """éƒ¨ç½²å…¥å£"""
    print("Deploying to Modal...")
    
    # æµ‹è¯•å‡½æ•°
    with open("test_image.jpg", "rb") as f:
        image_data = f.read()
    
    result = generate_video_modal.remote(
        image_data,
        {"intensity": 150}
    )
    
    print(f"Generated video size: {len(result)} bytes")
```

#### ä½¿ç”¨Runpodéƒ¨ç½²ï¼ˆæŒä¹…GPUï¼‰

```python
# deploy/runpod_deployment.py
import runpod
import os

# Runpodå¤„ç†å‡½æ•°
def handler(job):
    """Runpod serverlesså¤„ç†å‡½æ•°"""
    
    job_input = job["input"]
    image_url = job_input["image_url"]
    motion_params = job_input["motion_params"]
    
    try:
        # ç”Ÿæˆè§†é¢‘
        result = generate_video_local(image_url, motion_params)
        
        return {
            "output": result,
            "status": "success"
        }
    except Exception as e:
        return {
            "error": str(e),
            "status": "failed"
        }

def generate_video_local(image_url: str, motion_params: dict):
    """æœ¬åœ°GPUç”Ÿæˆå‡½æ•°"""
    
    # è¿™é‡Œè¿è¡Œå®é™…çš„æ¨¡å‹
    from diffusers import StableVideoDiffusionPipeline
    import torch
    
    # æ¨¡å‹ä¼šè¢«æŒä¹…åŒ–åœ¨podä¸­
    if not hasattr(generate_video_local, 'pipe'):
        generate_video_local.pipe = StableVideoDiffusionPipeline.from_pretrained(
            "stabilityai/stable-video-diffusion-img2vid-xt",
            torch_dtype=torch.float16
        ).to("cuda")
    
    pipe = generate_video_local.pipe
    
    # ç”Ÿæˆè§†é¢‘
    # ... å®é™…ç”Ÿæˆä»£ç 
    
    return video_url

# å¯åŠ¨RunpodæœåŠ¡
runpod.serverless.start({
    "handler": handler
})
```

### æˆæœ¬å¯¹æ¯”ä¸ç›‘æ§

```python
# services/cost_monitor.py
import time
from datetime import datetime, timedelta
from typing import Dict

class CostMonitor:
    """æˆæœ¬ç›‘æ§æœåŠ¡"""
    
    def __init__(self):
        self.costs = {
            "replicate": 0.001,  # æ¯ç§’
            "modal": 0.0003,     # æ¯ç§’
            "runpod": 0.0002,    # æ¯ç§’
            "self_hosted": 0.00005  # æ¯ç§’ï¼ˆä»…ç”µè´¹ï¼‰
        }
        self.usage_log = []
    
    def track_usage(
        self,
        service: str,
        duration_seconds: float,
        task_id: str
    ):
        """è®°å½•ä½¿ç”¨æƒ…å†µ"""
        cost = self.costs.get(service, 0) * duration_seconds
        
        self.usage_log.append({
            "timestamp": datetime.now(),
            "service": service,
            "duration": duration_seconds,
            "cost": cost,
            "task_id": task_id
        })
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢æœåŠ¡
        self.check_cost_optimization()
    
    def check_cost_optimization(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢åˆ°æ›´ä¾¿å®œçš„æœåŠ¡"""
        
        # è®¡ç®—æœ€è¿‘ä¸€å°æ—¶çš„æˆæœ¬
        one_hour_ago = datetime.now() - timedelta(hours=1)
        recent_usage = [
            log for log in self.usage_log 
            if log["timestamp"] > one_hour_ago
        ]
        
        total_cost = sum(log["cost"] for log in recent_usage)
        
        # å¦‚æœæˆæœ¬è¶…è¿‡é˜ˆå€¼ï¼Œåˆ‡æ¢æœåŠ¡
        if total_cost > 10:  # $10/å°æ—¶
            self.recommend_service_switch()
    
    def recommend_service_switch(self):
        """æ¨èæœåŠ¡åˆ‡æ¢"""
        print("âš ï¸ æˆæœ¬è­¦å‘Šï¼šå»ºè®®åˆ‡æ¢åˆ°è‡ªå»ºæœåŠ¡")
        # å‘é€é€šçŸ¥
        self.send_alert({
            "type": "cost_alert",
            "message": "Hourly cost exceeded $10",
            "recommendation": "Switch to self-hosted"
        })
    
    def get_cost_report(self, period_days: int = 7) -> Dict:
        """ç”Ÿæˆæˆæœ¬æŠ¥å‘Š"""
        cutoff = datetime.now() - timedelta(days=period_days)
        period_logs = [
            log for log in self.usage_log
            if log["timestamp"] > cutoff
        ]
        
        # æŒ‰æœåŠ¡åˆ†ç»„ç»Ÿè®¡
        service_costs = {}
        for log in period_logs:
            service = log["service"]
            if service not in service_costs:
                service_costs[service] = {
                    "count": 0,
                    "total_cost": 0,
                    "total_duration": 0
                }
            
            service_costs[service]["count"] += 1
            service_costs[service]["total_cost"] += log["cost"]
            service_costs[service]["total_duration"] += log["duration"]
        
        return {
            "period_days": period_days,
            "total_cost": sum(s["total_cost"] for s in service_costs.values()),
            "total_videos": sum(s["count"] for s in service_costs.values()),
            "by_service": service_costs,
            "recommendations": self.get_recommendations(service_costs)
        }
    
    def get_recommendations(self, service_costs: Dict) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        total_cost = sum(s["total_cost"] for s in service_costs.values())
        
        if total_cost > 100:
            recommendations.append("Consider self-hosting for cost reduction")
        
        if "replicate" in service_costs and service_costs["replicate"]["count"] > 1000:
            recommendations.append("High Replicate usage - switch to Modal for 70% cost reduction")
        
        return recommendations
```

---

## ğŸš€ Phase 4: è‡ªä¸»æŠ€æœ¯ï¼ˆç¬¬3-6ä¸ªæœˆï¼‰

### æ„å»ºè‡ªå·±çš„è§†é¢‘ç”ŸæˆPipeline

```python
# services/custom_pipeline.py
import torch
from diffusers import DiffusionPipeline, UNet2DConditionModel
from transformers import CLIPTextModel, CLIPTokenizer
import numpy as np

class CustomVideoPipeline:
    """è‡ªç ”è§†é¢‘ç”Ÿæˆç®¡çº¿"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.load_models()
    
    def load_models(self):
        """åŠ è½½å’Œé…ç½®æ¨¡å‹"""
        
        # åŸºç¡€æ¨¡å‹
        self.base_model = DiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-2-1",
            torch_dtype=torch.float16
        ).to(self.device)
        
        # è¿åŠ¨æ¨¡å—ï¼ˆè‡ªè®­ç»ƒæˆ–å¼€æºï¼‰
        self.motion_module = self.load_motion_module()
        
        # æ—¶åºä¸€è‡´æ€§æ¨¡å—
        self.temporal_module = self.load_temporal_module()
    
    def load_motion_module(self):
        """åŠ è½½è¿åŠ¨æ§åˆ¶æ¨¡å—"""
        # å¯ä»¥ä½¿ç”¨AnimateDiffçš„æƒé‡
        # æˆ–è€…è‡ªå·±è®­ç»ƒçš„æ¨¡å—
        pass
    
    def generate_video(
        self,
        image: Image.Image,
        motion_params: Dict,
        num_frames: int = 25
    ) -> List[np.ndarray]:
        """ç”Ÿæˆè§†é¢‘å¸§"""
        
        frames = []
        
        # ç¼–ç åˆå§‹å›¾åƒ
        latents = self.encode_image(image)
        
        for i in range(num_frames):
            # è®¡ç®—å½“å‰å¸§çš„è¿åŠ¨åç§»
            motion_offset = self.calculate_motion(motion_params, i / num_frames)
            
            # åº”ç”¨è¿åŠ¨åˆ°æ½œåœ¨ç©ºé—´
            moved_latents = self.apply_motion(latents, motion_offset)
            
            # è§£ç ä¸ºå›¾åƒ
            frame = self.decode_latents(moved_latents)
            
            # åº”ç”¨æ—¶åºä¸€è‡´æ€§
            if i > 0:
                frame = self.apply_temporal_consistency(frame, frames[-1])
            
            frames.append(frame)
        
        return frames
    
    def train_on_custom_data(self, dataset_path: str):
        """åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè®­ç»ƒ"""
        
        # åŠ è½½æ•°æ®é›†
        dataset = self.load_video_dataset(dataset_path)
        
        # è®­ç»ƒé…ç½®
        training_config = {
            "learning_rate": 1e-5,
            "batch_size": 4,
            "num_epochs": 100,
            "gradient_accumulation_steps": 4
        }
        
        # è®­ç»ƒå¾ªç¯
        optimizer = torch.optim.AdamW(
            self.motion_module.parameters(),
            lr=training_config["learning_rate"]
        )
        
        for epoch in range(training_config["num_epochs"]):
            for batch in dataset:
                # å‰å‘ä¼ æ’­
                loss = self.compute_loss(batch)
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ›´æ–°æƒé‡
                optimizer.step()
                optimizer.zero_grad()
                
                print(f"Epoch {epoch}, Loss: {loss.item()}")
```

---

## ğŸ“Š ç›‘æ§ä¸åˆ†æ

### å®æ—¶ç›‘æ§Dashboard

```python
# monitoring/metrics_dashboard.py
from fastapi import FastAPI, WebSocket
from datetime import datetime
import json
import asyncio

app = FastAPI()

class MetricsCollector:
    def __init__(self):
        self.metrics = {
            "total_generated": 0,
            "success_rate": 100.0,
            "avg_generation_time": 0,
            "active_tasks": 0,
            "cost_today": 0,
            "errors_today": []
        }
        self.websocket_clients = []
    
    async def update_metric(self, metric_name: str, value: any):
        """æ›´æ–°æŒ‡æ ‡"""
        self.metrics[metric_name] = value
        
        # å¹¿æ’­åˆ°æ‰€æœ‰WebSocketå®¢æˆ·ç«¯
        await self.broadcast_update()
    
    async def broadcast_update(self):
        """å¹¿æ’­æ›´æ–°åˆ°æ‰€æœ‰å®¢æˆ·ç«¯"""
        message = json.dumps({
            "timestamp": datetime.now().isoformat(),
            "metrics": self.metrics
        })
        
        for client in self.websocket_clients:
            await client.send_text(message)

metrics = MetricsCollector()

@app.websocket("/ws/metrics")
async def websocket_metrics(websocket: WebSocket):
    """WebSocketç«¯ç‚¹forå®æ—¶æŒ‡æ ‡"""
    await websocket.accept()
    metrics.websocket_clients.append(websocket)
    
    try:
        while True:
            # ä¿æŒè¿æ¥
            await websocket.receive_text()
    except:
        metrics.websocket_clients.remove(websocket)

@app.get("/api/metrics/summary")
async def get_metrics_summary():
    """è·å–æŒ‡æ ‡æ‘˜è¦"""
    return {
        "metrics": metrics.metrics,
        "timestamp": datetime.now().isoformat()
    }
```

---

## ğŸš¢ éƒ¨ç½²ä¸è¿ç»´

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

```yaml
# kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: draw-to-video-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: draw-to-video
  template:
    metadata:
      labels:
        app: draw-to-video
    spec:
      containers:
      - name: api
        image: drawtovideo/api:latest
        ports:
        - containerPort: 8000
        env:
        - name: REPLICATE_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: replicate-token
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
---
apiVersion: v1
kind: Service
metadata:
  name: draw-to-video-service
spec:
  selector:
    app: draw-to-video
  ports:
    - port: 80
      targetPort: 8000
  type: LoadBalancer
```

### CI/CD Pipeline

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run tests
        run: |
          pip install -r requirements.txt
          pytest tests/

  deploy:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Build Docker image
        run: |
          docker build -t drawtovideo/api:${{ github.sha }} .
          docker tag drawtovideo/api:${{ github.sha }} drawtovideo/api:latest
      
      - name: Push to registry
        run: |
          echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
          docker push drawtovideo/api:${{ github.sha }}
          docker push drawtovideo/api:latest
      
      - name: Deploy to Kubernetes
        run: |
          kubectl apply -f kubernetes/
          kubectl set image deployment/draw-to-video-api api=drawtovideo/api:${{ github.sha }}
```

---

## ğŸ’° æˆæœ¬ç›‘æ§ä¸ä¼˜åŒ–

### å®æ—¶æˆæœ¬è¿½è¸ª

```python
# cost_tracking.py
class CostTracker:
    def __init__(self):
        self.cost_per_service = {
            "replicate": {"per_second": 0.001, "total": 0},
            "modal": {"per_second": 0.0003, "total": 0},
            "self_hosted": {"per_second": 0.00005, "total": 0}
        }
        
    def get_daily_report(self):
        """æ¯æ—¥æˆæœ¬æŠ¥å‘Š"""
        return {
            "date": datetime.now().date(),
            "total_cost": sum(s["total"] for s in self.cost_per_service.values()),
            "by_service": self.cost_per_service,
            "projections": {
                "monthly": self.project_monthly_cost(),
                "yearly": self.project_yearly_cost()
            },
            "recommendations": self.get_cost_recommendations()
        }
```

---

## ğŸ“ˆ æˆåŠŸæŒ‡æ ‡

### KPIè¿½è¸ª

```python
kpis = {
    "week_1": {
        "videos_generated": 100,
        "success_rate": 95,
        "avg_generation_time": 15,
        "cost_per_video": 0.05
    },
    "week_2": {
        "videos_generated": 1000,
        "success_rate": 98,
        "avg_generation_time": 12,
        "cost_per_video": 0.03
    },
    "month_1": {
        "videos_generated": 10000,
        "success_rate": 99,
        "avg_generation_time": 10,
        "cost_per_video": 0.01
    },
    "month_3": {
        "videos_generated": 100000,
        "success_rate": 99.5,
        "avg_generation_time": 8,
        "cost_per_video": 0.005
    }
}
```

---

## âœ… å®æ–½æ£€æŸ¥æ¸…å•

### Phase 1 æ£€æŸ¥ç‚¹
- [ ] Replicate APIé›†æˆå®Œæˆ
- [ ] åŸºç¡€è·¯å¾„è½¬è¿åŠ¨ç®—æ³•å®ç°
- [ ] 3ç§åŸºç¡€æ•ˆæœå¯ç”¨
- [ ] è§†é¢‘ç”Ÿæˆæ—¶é—´ < 15ç§’
- [ ] æˆåŠŸç‡ > 90%

### Phase 2 æ£€æŸ¥ç‚¹
- [ ] ç¼“å­˜ç³»ç»Ÿè¿è¡Œæ­£å¸¸
- [ ] 5+ç§æ•ˆæœæ¨¡æ¿
- [ ] æ··åˆæ¨¡å‹æ–¹æ¡ˆæµ‹è¯•
- [ ] ç”Ÿæˆæ—¶é—´ < 10ç§’
- [ ] æˆåŠŸç‡ > 95%

### Phase 3 æ£€æŸ¥ç‚¹
- [ ] Modal/Runpodéƒ¨ç½²å®Œæˆ
- [ ] æˆæœ¬é™ä½ > 50%
- [ ] è‡ªåŠ¨æœåŠ¡åˆ‡æ¢é€»è¾‘
- [ ] å®æ—¶æˆæœ¬ç›‘æ§
- [ ] ç”Ÿæˆæ—¶é—´ < 8ç§’

### Phase 4 æ£€æŸ¥ç‚¹
- [ ] è‡ªç ”Pipelineè¿è¡Œ
- [ ] æˆæœ¬ < $0.01/è§†é¢‘
- [ ] è´¨é‡åŒ¹é…Higgsfield
- [ ] ç‹¬ç‰¹æ•ˆæœ3+ç§
- [ ] å®Œå…¨æŠ€æœ¯è‡ªä¸»

---

## ğŸ¯ æ€»ç»“

è¿™ä¸ªå®æ–½æ–¹æ¡ˆçš„æ ¸å¿ƒä¼˜åŠ¿ï¼š

1. **å¿«é€Ÿå¯åŠ¨** - ç¬¬ä¸€å‘¨å³å¯ä¸Šçº¿MVP
2. **é€æ­¥ä¼˜åŒ–** - ä»APIåˆ°è‡ªå»ºï¼Œå¹³æ»‘è¿‡æ¸¡
3. **æˆæœ¬å¯æ§** - å¤šç§æ–¹æ¡ˆï¼Œçµæ´»åˆ‡æ¢
4. **æŠ€æœ¯ç§¯ç´¯** - æœ€ç»ˆå®ç°æŠ€æœ¯ç‹¬ç«‹

è®°ä½ï¼š**å…ˆä¸Šçº¿ï¼Œå†ä¼˜åŒ–**ã€‚ä¸è¦ä¸€å¼€å§‹å°±è¿½æ±‚å®Œç¾ï¼Œè€Œæ˜¯å¿«é€ŸéªŒè¯å¸‚åœºï¼Œç„¶åæŒç»­æ”¹è¿›ã€‚

---

*æ–‡æ¡£ç‰ˆæœ¬ï¼š2.0*
*æ›´æ–°æ—¥æœŸï¼š2025å¹´1æœˆ14æ—¥*
*é¢„è®¡å®Œæˆæ—¶é—´ï¼š3-6ä¸ªæœˆè¾¾åˆ°æŠ€æœ¯ç‹¬ç«‹*